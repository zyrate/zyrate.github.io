{"meta":{"title":"zyrate's Blog","subtitle":"为者常成，行者常至","description":"","author":"Yunrui Zheng","url":"https://zyrate.github.io","root":"/"},"pages":[],"posts":[{"title":"MIT6.824 分布式系统课程实验笔记 Lab 2","slug":"824 Lab 2","date":"2024-01-04T16:00:00.000Z","updated":"2024-03-12T10:00:04.235Z","comments":true,"path":"2024/01/05/824 Lab 2/","link":"","permalink":"https://zyrate.github.io/2024/01/05/824%20Lab%202/","excerpt":"","text":"Lab 2、3、4 是一个系列，最终需要实现一个可容错、可分片的分布式 K-V 数据库，该数据库的底层基于分布式共识算法 Raft。Lab 2 的目标就是实现 Raft 底层协议，以支持数据库的分布式、多副本、一致性的需求。 Raft 算法Raft 算法的提出就是为了解决之前的分布式共识算法 Paxos 可理解性较差、不易实现的问题。但即便如此，Raft 算法理解、实现起来也并不简单，并且有很多需要注意的细节问题。Raft 算法设计的目的是什么？本节先从分布式副本集群、主备同步、分布式共识等几个方面进行简单的梳理。 多副本集群多副本集群是分布式系统能够正常运行的基础。我们构建分布式系统可能有多个原因，比较主要的两个原因就是：并行计算和容错。这两者之间是有一定因果关系的，如下面的逻辑推演： 单体应用想要提高性能 ——&gt; 并行计算 并行计算需要将数据划分后同时进行 ——&gt; 构建服务器集群 构建集群后不可避免地会出现单点故障 ——&gt; 使集群能够自动容错 容错要求出现故障后数据不丢失 ——&gt; 每个数据复制多个副本 数据的多个副本必须保持一致性 –&gt; 每个副本之间需要进行同步 副本同步带来额外的开销 –&gt; 降低了整体性能 可以看到，最终的结果可能和最初的设想有些矛盾，也就是说为了保证多副本之间的一致性，就必须牺牲掉一定的性能。分布式系统的设计需要在它们之间进行权衡，在能够达到可接受的容错性的同时减少性能的损耗。 主备同步关于如何设计多副本系统，有很多不同的实现，但是基本都离不开主备同步。 也就是有一份主数据（Primary）和多份备份数据（Backup），这里的“数据”是抽象层面的，可以是任何需要多副本的对象，也可代表承载对象的服务器节点。平时我们处理和更新 Primary，同时让 Backup 与 Primary 保持同步，当 Primary 所在的节点故障后可以立刻启用 Backup，而不用等待故障恢复。（需要注意的是，外部调用者不应当感知到数据有多个副本） 关于主备之间的同步，有两种方案： State transfer，状态转移。Primary 将自己完整状态，比如说内存中的内容，拷贝并发送给 Backup。Backup 会保存收到的最近一次状态。 Replicated state machine，复制状态机。使服务器状态发生改变的往往是外部事件，这个方案不会在不同的副本之间发送完整状态，只会从 Primary 将这些外部事件，例如外部的输入、增删改查，发送给 Backup，从而达到状态同步。 上面两种方案人们更亲向于后者，因为需要传递的数据更少，但是它实现起来更复杂一些，也会受并行计算的影响（比如在多核处理器上，同样的外部命令可能得到不同的结果，如随机数）。 不论哪种主备同步，都需要进行良好的输出控制（Output Rule），防止出现外部更新了 Primary，副本因为网络问题没有及时同步数据，而外部已经收到了更新结果的情况。所以大部分的分布式副本系统都涉及到 Primary 停下来等待 Backup 的问题。Raft 算法也不例外。 分布式共识在多副本集群中，Primary 应当只有一个，否则会出现脑裂。但是多个节点究竟让谁作 Primary 呢？不同的设计思想可能有不同的实现，但是不管怎样只要是通过单服务去决定谁是 Primary 就是有问题的，因为它又会引出单点故障，是不可靠的，哪怕这个服务本身也是多副本。比如下面的设计思想： 多个副本通过Test-And-Set服务（TAS 服务本身也是多副本的）去请求当 Primary（类似请求获取分布式锁），最终应该只有一个请求成功。但是在分布式环境下网络是不可靠的，可能会出现下面的情况： 如果有两个客户端 C1，C2 请求成为主副本，有两个 TAS 服务器 S1，S2。当一个客户端能和一个服务器通信而不能和另外一个通信的时候，就可能会发生脑裂问题。此时客户端有两种选择： 依次询问所有服务器 S1、S2，等待它们的响应。但这样就失去了容错的意义，因为只要有一台服务器坏掉，系统就无法正常运行；而且，一个好的多副本服务是不应该让客户端意识到有多个副本的。 认为自己无法与其通信的服务器 S2 已经宕机了，所以其他服务器也一定发现了这个问题，所以可以只去 S1 服务器请求。但是这个想法是错误的，因为 S2 很可能没有宕机，而是由于网络问题， C1 只能通信 S1，C2 只能通信 S2，这样很容易就形成脑裂了。 虽然上面的设计思想在现实生活中可以较好地运行，只要保证集群网络正常就行，但是毕竟是不完美的。 因此，像 Paxos、Raft、ZAB 这类靠“ 过半票决 ”思想来保证主备一致性的分布式共识算法就被提了出来。 Raft 算法概述Raft 算法本身是一个复制状态机架构，其中的外部事件它称为日志（log），日志是有时间顺序的，不同的节点之间只要日志是一致的，那么状态就是一致的。所以 Raft 算法最重要的部分就是日志的正确同步。 image.png 图中的State Machine部分其实就是需要保证分布式一致性的数据，在不同的应用中指代的对象可能是不同的。比如在分布式 KV 数据库中，它可能存储的就是键和值；在分布式消息队列中，它可能存储的就是消息的状态。Raft 算法并不知道每一条 Log 代表的含义，也不知道状态机目前的状态，它只保证不同节点之间的状态机是一致的。所以 Raft 算法处于一个所有应用共性的位置，也就是上图②和③所代表的流程。 Raft 算法解决了三个子问题：领导者选举、日志复制、安全性。 领导者选举每个节点可能的状态：follower, candidate, leader leader 的作用是协调所有的 follower 进行正确的日志同步，并响应上层的事件输入。它通过心跳来在集群中维护自己的“统治地位”。但是如何避免上文所述的单点故障呢？如果 leader 断连或者宕机了，那么需要在剩下的 follower 中选择一个新的 leader，选举的规则就是某一个 follower 转变为 candidate，然后让其他节点为自己投票，如果所获同意的票数超过节点总数（包括宕机和断连的）的一半（大多数），那么该节点就成为了新的 leader。为了区分新旧 leader ，Raft 采用了任期（Term）的概念，所有旧 Term 的消息都应当被舍弃。在同一个 Term 中，一个 follower 只能给一个 candidate 投票。 为了尽量避免选举分裂的情况，Raft 规定每个 follower 在收不到 leader 心跳的随机一段时间后发起新一轮选举。节点的状态转换如下图： image.png 在 Raft 中，只要 Term 合法，follower 对于 leader 必须无条件服从，所以对 candidate 的投票必须谨慎，因为投票规则决定了数据的安全性。 日志复制日志复制是 Raft 算法中最重要的部分，需要保证无论是 follower 还是 leader 在宕机重启后经过一定的机制使得日志重新一致。日志的产生来自于客户端（外部）的命令，客户端与 leader 进行交互，它向 leader 发出一个命令，leader 产生一个日志，然后 leader 向 follower 进行同步，当集群中超过半数的 follower 都收到日志后，leader 向客户端通知操作成功。 这有点像两阶段提交（2 PC） 但并不完全是，这里的提交指的是集群的一种状态，也就是某个日志如果达成了群体一致性，那么集群就可以将这个日志的状态设置为已提交，leader 向状态机 apply 某个操作进而通知客户端操作成功的前提是该日志已经提交。 如果 leader 和 follower 都不出错，并且网络正常，那么底层 Raft 的流程就是下面的步骤： leader 接收客户端操作命令 leader 将命令转化为一条日志追加到本地 leader 向所有的 follower 发起日志追加请求 follower 收到请求后追加新日志到本地，返回成功与否 leader 在收到过半的 follower 追加成功的答复后，将本地该日志设置为已提交，并向上层状态机 apply leader 状态机返回客户端此次操作的结果 leader 通过心跳向 follower 广播该日志的提交状态 follower 收到心跳同样将本地的该日志设置为已提交，并向上层状态机 apply 这样一来，所有节点的本地日志、状态机状态都是一致的。但是，分布式环境下 leader、follower、网络状态都可能频繁出错，所以实际的日志复制情况并没有这么简单，需要考虑数据的恢复，并保证安全性和一致性。 安全性Raft 需要一定的规则来保证数据的安全性和一致性。 安全性主要指客户端已经收到处理成功的操作不能因为集群的某些节点宕机而丢失或被覆盖。具体来说的话可以分为：投票安全性、复制安全性、提交安全性。 投票安全性： follower 在每个 Term 只能对一个 candidate 投票 不能选日志的 Term 和 Index 落后自己的节点作为 leader 复制安全性： follower 在收到 leader 的日志追加请求时如果位置冲突则需要通知 leader 进行快速恢复（从不冲突的位置进行覆盖），这样能保证在新的 leader 上任后，所有的 follower 和 leader 的日志一致，并且没有空隙 提交安全性： 因为一个节点一旦提交、apply 后就不能反悔了，所以提交也需要慎重 leader 需要在合适的时机确定所有的 follower 都已经完成同步的日志号，并且该日志的 Term 需要等于当前 Term，然后将该位置的日志提交 follower 只有在收到 leader 的提交位置后才能更新自己的提交位置 一致性主要指集群中所有的节点数据需要能够容错，在恢复后仍然能保持一致。从整体来看，Raft 算法是强一致性模型，也就是系统保证不论在哪个时间点，外部的每个读操作都将返回最近的写操作的结果。保证强一致的机制就是 Raft 的集群提交状态，只有 leader 收到过半的 follower 的同步成功后，才向上层返回结果，此时哪怕 leader 宕机了，上面的投票规则也能保证选举出来的新 leader 具有最新的日志。 Lab 要求Lab 2 有四个子 Lab： Lab 2A：实现 leader 选举 Lab 2B：实现日志复制/同步 Lab 2C：实现持久化 Lab 2D：实现日志压缩（快照） 每一个子 Lab 都有一系列测试。2A 和 2B 是最主要的实现，2C 和 2D 以这两个为基础。但是 2C 和 2D 的测试极其严苛，能够测出很多前两个 Lab 中存在的 BUG，所以这四个 Lab 基本上是难度递增。 实验提供了 labrpc.go 用来模拟现实网络环境下的 RPC 调用，同时也可以产生很严苛的测试场景。我们的所有代码写在 raft.go 中，这个文件仅规定了 Raft 与外界交互的接口，其他的内部实现全部需要自己完成。 实验层会调用该文件中 Make(peers, me, ...) 函数来生成一个 Raft 对象，通过这个函数的参数我们可以知道集群中总共有多少个节点 (peers)，自己是哪一个节点（me），还有与外界传递信息的 apply 通道（applyCh）等。 实验层会调用 GetState() 方法来获取当前节点所处的状态（leader/follower/candidate），通常情况下，客户端只会与 leader 进行交互。 关键问题多 Leader 错误一般来说，只要不出现网络分区情况，集群中只能有一个 Leader，否则就是脑裂了。这就要求节点的身份及时、正确地改变。任何节点在收到 AE 和 Vote 请求时都要根据 Term 去判断自己是否要改变身份，这里身份的改变和其他属性（如当前 Term）的改变要保持原子性，否则容易出现多 Leader 错误。 计时功能设计Lab 官方推荐在需要进行周期性动作的地方使用 time.Sleep() 函数实现（因为简单），但是这个方案对于 Raft 的超时选举算法并不优雅，因为 Raft 要求 follower 在每次收到 leader 的心跳后重置选举计时器。因此我采用了 time.Timer 来实现 follower 的选举计时和 leader 的心跳计时，虽然 debug 难度增加了，但是更加接近 Raft 设计者的本意。 Log 存储问题从逻辑上看，每个节点的本地日志就是一个 log 数组，一开始我也仅仅用一个数组存储，但是 log Index 的下标是从 1 开始的（论文中的考量），这就需要判断好各种边界问题。此外，做到 Lab 2D 的时候要求对日志进行裁剪，这个时候逻辑 Index 和实际 Index 已经完全不同了，如果每次都额外判断、处理的话工作量太大。所以我采用了面向对象的思想，引入了 LogManager 对象，用于统一向外界提供从 1 开始的索引映射和增删改查功能，大大的方便了后续开发。 一致性检查的问题Leader 始终要保证自己的 follower 和自己的 log 内容是一模一样的，同步就是通过 AE 请求。如果 Leader 发现了不一致的现象就要及时纠正。但是，检查的时机如何确定，如果次数太少，会导致群体一致性达成过慢；如果太过频繁，会加大并发问题的概率，同时也会造成网络拥堵。一般来说，每次心跳都是一次同步，但是远远不够，因为客户端可能会在两次心跳之间多次操作，请求达成一致。因此需要一种机制，既能够及时响应 Command，又能够尽量减少并发 RPC 冲突。 我采用的是清零心跳计时器的方法。计时器清零后，如果上一次心跳已经结束，那么下一次会马上开始；如果上一次还没结束，那么就会等待心跳全部发送出去。这样不会再短时间的高并发情况下导致过多的心跳 RPC 出现，同时也能够达到一种批处理的效果。 如何判断提交状态一开始，我是在每次心跳完成后，根据 follower 的回复判断是否能够更新提交，后来发现太天真了。论文中设置 matchIndex 是非常有必要的，需要另起一个协程，通过判断 matchIndex 和 Leader 的 log index 来判断当前的提交位置。并且有一个条件非常关键：Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目，否则在宕机重启后会出现数据不一致。 持久化需要注意什么在分布式开发中，任何时刻都要确保系统在某一时刻的状态是一致的，因此在状态更改的时候为了防止宕机丢失数据，一定要及时进行持久化。还要搞清两个问题： 为什么要持久化 currentTerm？ 因为避免出现两个意义不同的但数字相同的 Term 为什么要持久化 votedFor？ 因为要避免一个 server 同时给多个 Candidate 投票 快照需要注意什么在论文中有两种方法进行快照，一种是每个服务器独立拍摄快照的；另一种是由 leader 领导拍摄。Lab 是第一种，因此不用由 leader 主动同步，而是发现 follower 过于落后时同步。每个服务器平时也不需要 apply 快照，而是在过于落后或重启时才 apply。 因为快照请求有可能在任何时间到达，它和 apply 操作是互斥的，因此一定要保证先后顺序，不让多余的 log 向上层 apply。 分布式应用开发的难点最难的部分就是不确定性，状态、请求顺序、机器可靠等问题都有极大地不确定性，需要很多看起来冗余实际上非常有必要的操作去避免不确定性带来的数据不一致问题，比如代码的先后顺序、原子性保证等。 分布式应用的调试会非常折磨，因为有些 BUG 无法快速复现，因此在开发是需要养成随时打印日志的习惯，以便后期排查。 具体实现Raft 对象结构Raft 从实现层面来说是一个底层库，向上层（Server）提供一致性保障的方法，因此需要明了 Raft 向外提供什么接口： Make(peers, me, persister, applyCh)，在当前 Server 节点构建 Raft 对象并返回，peers 是集群节点信息，me 是当前节点编号，persister 是持久化工具类，applyCh 是 Server 接收某个命令达成一致性结果的消息通道； Start(command interface&#123;&#125;)，向当前节点发起一条命令一致性过程，该方法只有处于 Leader 状态的节点才可被调用； GetState()，获取 Server 当前的 Raft 状态； Kill()，结束 Raft 服务； Snapshot(index int, snapshot []byte)，Server 完成快照，向 Raft 同步（日志裁剪） 我设计的 Raft 对象关键字段： 123456789101112131415161718192021// 持久属性currentTerm int // 当前任期votedFor int // 当前任期投票给了谁lm *LogManager // 管理日志存储snapshot []byte // 上一次保存的快照// 易失属性state string // 当前角色状态commitIndex int // 已提交的最大下标lastApplied int // 以应用到状态机的最大下标applyCh chan ApplyMsg // apply通道applyCond *sync.Cond // apply协程唤醒条件installSnapCh chan int // install snapshot的信号通道，传入trim indexbackupApplied bool // 从磁盘恢复的snapshot已经applynotTicking chan bool // 没有进行选举计时electionTimer *time.Timer // 选举计时器// Leader特有字段nextIndexes []int // 对于每个follower，leader要发送的下一个复制日志下标matchIndexes []int // 已知每个follower和自己一致的最大日志下标heartbeatTimer *time.Timer // 心跳计时器 易失属性是指断电重启后就丢失内容的属性，丢失后不影响状态的正确性；持久属性是必须在适当的时间进行持久化的属性，否则断电重启后集群会发生异常。 超时选举实现在集群刚启动的时候，所有的节点都是 follower 状态，需要节点自觉地发起选举。为了避免同一个时刻有多个节点发起选举造成选举分裂，采用随机超时策略，当一个节点在某个随机的时间内都没有收到心跳的话，他便发起新一轮选举。选举流程： 当前 Term+1 转换状态为 candidate 投自己一票 向其他人广播选举请求 当收到超过半数投票后，转换为 leader，开启心跳 投票通过 RequestVoteRPC 进行通知 1234567891011121314151617181920212223242526272829303132333435363738func (rf *Raft) kickOffElection() &#123; rf.currentTerm++ // 更新任期 rf.state = CANDIDATE // 变为候选人 rf.votedFor = rf.me // 先投自己一票 rf.persist() votes := 1 total := len(rf.peers) args := rf.genRequestVoteArgs() mu := sync.Mutex&#123;&#125; for i := 0; i &lt; total; i++ &#123; if i == rf.me &#123; continue &#125; go func(server int) &#123; reply := RequestVoteReply&#123;&#125; if !rf.sendRequestVote(server, &amp;args, &amp;reply) &#123; return &#125; if args.Term == rf.currentTerm &amp;&amp; rf.state == CANDIDATE &#123; // 防止等的时间太长，已经开始了下一轮，或者身份已经变化 if reply.Term &lt; rf.currentTerm &#123; // 投票者的任期小于自己的 // 丢弃 return &#125; if reply.VoteGranted &#123; mu.Lock() votes++ if rf.state != LEADER &amp;&amp; votes &gt; total/2 &#123; // 得票超半数 rf.initLeader() rf.state = LEADER // 立即开始心跳 go rf.heartbeat() &#125; mu.Unlock() &#125; &#125; &#125;(i) &#125;&#125; 需要注意选举过程中身份、任期的判断和互斥锁的使用。 投票流程：任何节点在接收到某个非旧任期 candidate 的投票请求后都要有所响应，要么同意要么否决。需要根据安全性规则投票。 12345678910111213141516171819202122232425262728293031323334func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) &#123; if args.Term &gt;= rf.currentTerm &#123; // 候选人要比自己所在任期领先或持平 if args.Term &gt; rf.currentTerm &#123; // 新任期重置投票记录,改变身份 rf.enterNewTerm(args.Term) &#125; if rf.state != CANDIDATE &#123; // 如果不是候选人就重置选举计时 rf.resetElectionTimer() &#125; rf.mu.Lock() reply.Term = rf.currentTerm reply.VoteGranted = false if rf.votedFor == -1 || rf.votedFor == args.CandidateId &#123; // 每个任期只能投一次票 // 安全性检查，即不能选日志的term和index落后自己的节点作为leader，term优先级大于index len := rf.lm.len() var lastLogIndex, lastLogTerm int // 自身的最后index和term if len == rf.lm.lastTrimmedIndex &#123; lastLogIndex = rf.lm.lastTrimmedIndex lastLogTerm = rf.lm.lastTrimmedTerm &#125; else &#123; lastLogIndex = len lastLogTerm = rf.lm.get(len).Term &#125; if lastLogTerm == 0 || lastLogTerm &lt; args.LastLogTerm &#123; // 比较log term reply.VoteGranted = true rf.votedFor = args.CandidateId &#125; else if lastLogTerm == args.LastLogTerm &amp;&amp; lastLogIndex &lt;= args.LastLogIndex &#123; // log term一样，比较index reply.VoteGranted = true rf.votedFor = args.CandidateId &#125; &#125; rf.mu.Unlock() &#125; go rf.persist() // 返回前持久化&#125; 因为这个部分涉及到了之后的日志剪裁，所以代码稍复杂。 Leader 心跳实现当选成功的 Leader 需要立即发起心跳，心跳的目的有两个： 维护自己的统治地位 对 follower 进行日志同步 均通过 AppendEntriesRPC 进行通知 当 Entry 为空时，是心跳 当 Entry 不为空时，是日志同步12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (rf *Raft) sendHeartbeats() &#123; total := len(rf.peers) args := AppendEntriesArgs&#123;&#125; args.Term = rf.currentTerm args.LeaderId = rf.me args.LeaderCommit = rf.commitIndex // follower在收到日志后首先要检查这两个参数，与自己的不一致的话就要返回false // 这里是将leader目前最高index发送给follower去比对一致性，注意边界判断 rf.mu.Lock() if rf.lm.len() &gt;= 1 &#123; args.PrevLogIndex = rf.lm.len() if args.PrevLogIndex &gt; rf.lm.lastTrimmedIndex &#123; args.PrevLogTerm = rf.lm.get(args.PrevLogIndex).Term &#125; else &#123; args.PrevLogIndex = rf.lm.lastTrimmedIndex args.PrevLogTerm = rf.lm.lastTrimmedTerm &#125; &#125; rf.mu.Unlock() for i := 0; !rf.killed() &amp;&amp; i &lt; total &amp;&amp; rf.state == LEADER; i++ &#123; if i != rf.me &#123; go func(server int) &#123; reply := AppendEntriesReply&#123;&#125; if rf.state == LEADER &amp;&amp; rf.sendRequestAppendEntries(server, &amp;args, &amp;reply) &#123; if reply.Term &gt; rf.currentTerm &#123; // 其他节点的Term比自己高了，转变为follower rf.enterNewTerm(reply.Term) rf.resetElectionTimer() return &#125; if !reply.Success &#123; // 说明出现了日志不一致 // 主发生变化时有可能Index异常，因此先修正一次 rf.fastBackup(server, reply) rf.agreement(server, rf.lm.len(), false) // 从最新位置检查一致性 &#125; else &#123; // 更新一致的位置 rf.mu.Lock() rf.nextIndexes[server] = args.PrevLogIndex + 1 rf.matchIndexes[server] = args.PrevLogIndex rf.mu.Unlock() rf.tryCommit() // 每次心跳成功后检查能否进行提交 &#125; &#125; &#125;(i) &#125; &#125;&#125; 该方法通过定时器，定时执行。Leader 会拿自己当前最新的 log 的位置去询问所有的 follower 该位置是否冲突，如果不冲突，那么在 Leader 记录该 follower 与自己的 log 最大匹配的下标和下一个接收日志的下标；如果冲突，那么就需要进入快速恢复和同步的流程。 快速恢复和同步实现在 Leader 察觉到有 follower 与自己的 log 有冲突时，需要进行同步。因为 Leader 是从自己当前的最高 log 位询问 follower 的，所以 Leader 需要一步一步把 log 位置向前移再次询问，直到遇到一个 follower 不冲突的位置，那么从这个位置往后都要被 Leader 的日志所覆盖。这也是强一致性的体现。 但是，一个 follower 有可能刚刚宕机重启，它的日志可能落后于 Leader 很多条，这时候一步一步的试探效率十分低下。因此我们采用快速恢复策略，即 Leader 每次以 Term 为单位退回而不是 Index。 具体的做法就是 follower 在拒绝 Leader 的 AE 请求时在回应中添加一个额外的字段，让 Leader 能够判断如何快速回退： XTerm：冲突的 log 任期，如果不存在 log，返回-1 XIndex：任期号为 XTerm 的第一条 Log 的 Index XLen：follower 自己的 log 长度 Leader 端的快速恢复算法： 12345678910111213141516func (rf *Raft) fastBackup(server int, reply AppendEntriesReply) &#123; rf.mu.Lock() defer rf.mu.Unlock() if reply.XTerm == -1 &#123; // follower对应位置没有log rf.nextIndexes[server] = reply.XLen + 1 &#125; else if reply.XTerm == 0 &#123; // 某个地方出问题了，执行到这里不应该是0 rf.nextIndexes[server] = 1 // 重置为1总没错 &#125; else &#123; lastOfTerm := rf.findLastLogIndexOfTerm(reply.XTerm) if lastOfTerm == -1 &#123; // leader没有follower的Term rf.nextIndexes[server] = reply.XIndex &#125; else &#123; rf.nextIndexes[server] = lastOfTerm + 1 &#125; &#125;&#125; Leader 提交判断一开始，我们总会容易认为以 cmd 为单位进行判断，因为 Raft 的逻辑模型很明显：Leader 收到 cmd -&gt; 生成日志 -&gt; 集群复制该日志 -&gt; 超过半数成功 -&gt; 提交该日志 -&gt; apply 该 cmd 但其实，如果出现宕机重启、重新选举等问题，没有外部命令的时候我们同样需要不断更新提交状态，所以需要再每次同步之后，通过 Leader 记录的 matchIndex 来进行判断。需要注意：Raft 中的提交并不是某个操作，而是一种集群状态，如果 index 位置设置为提交位置，那么说明 index 之前的所有日志都已经提交。 123456789101112131415161718192021// leader统计matchIndex，尝试提交func (rf *Raft) tryCommit() &#123; total := len(rf.peers) // 找到一个最大的N&gt;commitIndex，使得超过半数的follower的matchIndex大于等于N， // 且leader自己N位置的log的Term等于当前Term（这一点很重要，安全性问题），那么N的位置就可以提交 len := rf.lm.len() for N := rf.commitIndex + 1; N &lt;= len; N++ &#123; majorityCnt := 1 for _, matchIndex := range rf.matchIndexes &#123; if matchIndex &gt;= N &amp;&amp; rf.lm.get(N).Term == rf.currentTerm &#123; majorityCnt++ &#125; &#125; if majorityCnt &gt; total/2 &#123; rf.commitIndex = N // 更新提交Index &#125; &#125; rf.mu.Lock() rf.applyCond.Signal() // 唤醒异步apply rf.mu.Unlock()&#125; Leader 的 commitIndex 会随着心跳和同步请求到达所有的 follower，使 follower 的 commitIndex 也随之更新。当任何节点的 commitIndex 领先于上一次 apply 的 Index 的时候，就可以将这部分 log 向上层进行 apply。 Follower 处理日志同步follower 对 Leader 发来的日志同步请求进行处理是整个 Raft 算法最核心的地方，因为他决定着 Raft 的同步机制是否能够保持数据一致性。在实现的时候需要遵循安全性规则并且要注意不确定性情况的发生。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func (rf *Raft) RequestAppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) &#123; reply.Term = rf.currentTerm if args.Term &lt; rf.currentTerm &#123; // 说明这个leader已经过期 reply.Success = false return &#125; // 进入新的一个Term，更新 if args.Term &gt; rf.currentTerm &#123; rf.enterNewTerm(args.Term) // 改变votedFor &#125; else &#123; rf.state = FOLLOWER // 不改变votedFor &#125; rf.resetElectionTimer() // 刷新选举计时 rf.mu.Lock() defer rf.mu.Unlock() if args.PrevLogIndex &lt; rf.commitIndex || rf.findLastLogIndexOfTerm(args.Term) &gt; args.PrevLogIndex &#123; // 说明这个请求滞后了 reply.XLen = rf.lm.len() reply.XTerm = -1 reply.Success = false return &#125; // 一致性检查 if args.PrevLogIndex &gt; rf.lm.len() &#123; reply.XLen = rf.lm.len() reply.XTerm = -1 reply.Success = false return &#125; if args.PrevLogIndex &gt;= 1 &amp;&amp; args.PrevLogIndex &gt; rf.lm.lastTrimmedIndex &#123; // 如果prevIndex已经被裁剪了，那一定不冲突 if rf.lm.get(args.PrevLogIndex).Term != args.PrevLogTerm &#123; // 有冲突了 reply.XTerm = rf.lm.get(args.PrevLogIndex).Term reply.XIndex = rf.findFirstLogIndexOfTerm(reply.XTerm) reply.Success = false return &#125; &#125; // 这里有可能leader传来的一部分log已经裁掉了，需要过滤一下 from := max(args.PrevLogIndex+1, rf.lm.lastTrimmedIndex+1) filter := min(from-args.PrevLogIndex-1, len(args.Entries)) // 防止越界 args.Entries = args.Entries[filter:] rf.lm.appendFrom(from, args.Entries) // 强制追加（覆盖）日志 // 提交log if args.LeaderCommit &gt; rf.commitIndex &#123; rf.commitIndex = min(args.LeaderCommit, rf.lm.len()) &#125; rf.applyCond.Signal() // 唤醒异步apply rf.persist() reply.Success = true&#125; 需要考虑选举计时刷新、滞后请求忽略、支持快速恢复、日志一致性检查、日志剪裁边界处理、更新提交 Index 几个方面。 异步 apply 实现因为 commitIndex 是随时动态变化的，所以 apply 也要跟着随时执行。但是在一个 Raft 内部同一时刻只能有一个 apply 线程，否则会发生冲突。我采用的是异步唤醒机制而不是方法调用去进行 apply。在开启 Raft 之后会新建一个 apply 协程，他会不断地比较 lastApplied 和 commitIndex 的大小，如果前者大于等于后者，说明当前没有需要 apply 的 cmd，协程阻塞；一旦 commitIndex 领先了，更新 commitIndex 的线程会唤醒该协程让他去 apply。这样就算同时有多个线程唤醒 apply，也能够保证幂等性，不会出现重复 apply 的现象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func (rf *Raft) apply() &#123; // 先apply快照 if rf.lm.lastTrimmedIndex != 0 &#123; rf.applySnapshot() &#125; rf.backupApplied = true // 然后再apply剩余log for !rf.killed() &#123; for rf.lastApplied &gt;= rf.commitIndex &#123; // 每次休眠前先看有无快照可apply select &#123; case index := &lt;-rf.installSnapCh: // 这两个操作要保证原子性 rf.trim(index) rf.applySnapshot() default: &#125; rf.mu.Lock() rf.applyCond.Wait() // 等待别处唤醒去apply，避免了并发冲突 rf.mu.Unlock() &#125; rf.mu.Lock() // commitIndex领先了 applyIndex := rf.lastApplied + 1 commitIndex := rf.commitIndex entries := rf.lm.split(applyIndex, commitIndex+1) // 本轮要apply的所有log rf.mu.Unlock() for index, log := range entries &#123; if applyIndex &lt;= rf.lm.lastTrimmedIndex &#123; // applyIndex落后快照了 break &#125; msg := ApplyMsg&#123; CommandValid: true, Command: log.Command, CommandIndex: applyIndex, CommandTerm: log.Term, // 为了Lab3加的 &#125; rf.applyCh &lt;- msg rf.mu.Lock() if rf.lastApplied &gt; applyIndex &#123; // 说明snapshot抢先一步了 rf.mu.Unlock() break &#125; rf.lastApplied = applyIndex applyIndex++ rf.mu.Unlock() &#125; &#125;&#125; 在 apply 的过程中需要注意快照的问题，因为快照随时可能到达，快照在 apply 之后不能够在 apply 快照之前的 cmd。此外，宕机重启后如果有已经备份快照也需要先 apply，因此我用了一个 backupApplied 去进行标识。 全部测试通过： image.png","categories":[],"tags":[{"name":"go","slug":"go","permalink":"https://zyrate.github.io/tags/go/"},{"name":"mit6824","slug":"mit6824","permalink":"https://zyrate.github.io/tags/mit6824/"},{"name":"分布式","slug":"分布式","permalink":"https://zyrate.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"MIT6.824 分布式系统课程实验笔记 Lab 1","slug":"824 Lab 1","date":"2023-11-30T16:00:00.000Z","updated":"2024-03-12T10:00:01.114Z","comments":true,"path":"2023/12/01/824 Lab 1/","link":"","permalink":"https://zyrate.github.io/2023/12/01/824%20Lab%201/","excerpt":"","text":"MIT 6.824 是麻省理工大学的一门研究生课程——Distributed Systems，学习这门课程对于了解分布式系统的构建原理、理解分布式程序的运行、优化分布式程序的运行环境会有很大的帮助。课程内容涵盖：分布式、容错、多副本、一致性等议题，附带了 4 个大的实验 Lab 并配套了相关的测试用例，需要基于 Go 语言完成。Lab 会将课程所讲的知识进行实践、贯通，有助于加深我们的理解和记忆。 做前准备网上对于本课程 Lab 的实现很多，但是大部分都是上来就讲原理和代码，很少有提到如何从头开始，而第一步往往是最难的。下面是一些准备工作和注意事项： 首先，这门课程的 Lab 是完全用 Go 写的，Lecture 也会时不时的讲到 Go 代码，所以需要提前熟悉 Go 的一些基础知识。语言只是一个工具，不必望而却步，只要之前能熟练使用 Java、C++等任何一门编程语言的都可以很快上手 Go，对于完成这门课程足够用了。 MIT 6.824 课程之所以很出名，原因之一就是其主讲人是 Robert Morris 教授，这是一个传奇大佬，课讲得很好。但似乎 2020 年以后他就不讲这门课了，所以我听的就是他的 2020 年课程，B 站资源：2020 MIT 6.824 分布式系统_哔哩哔哩_bilibili，目前网上免费资源大部分都是机翻字幕，有一些人工翻译的但并不完全。有个网站 https://www.simtoco.com/ 可以付费购买全部翻译课程，质量挺不错的。 关于 Lab 资源，我做的是 2022 年的版本，因为 2020 年项目的 Go 版本有些落后，不过其实都无所谓，每一年的 Lab 内容基本上是一样的。网址：6.824 Home Page: Spring 2022 (mit.edu) 可以先把项目 clone 下来（在 Lab 1 页面的开头有介绍），这门课程要求学习者不得把自己的仓库公开，以免出现作弊情况，所以如果要 push 到 GitHub 上的话记得把仓库设为私有。 接下来就可以开始先看 Lecture 了，总共有 20 个 Lec，每个 Lec 前一般都会分配一篇论文阅读（在 Schedule 页面），尽量先读过一遍。其实看完第一课就可以做 Lab 1 了，实现一个简易的分布式 MapReduce 只需要使用 RPC 和一定的容错（虽然并不容易），用不到多副本、一致性等内容。Lab 1 更像是一次牛刀小试，Lab 2~4 才会用到课程所授的大部分知识。 每个 Lab 提供了一些框架性的代码，需要自己编写关键代码，然后通过测试。具体需要在哪里动笔，课程网页都提供了详细的说明。比如 Lab 1，他已经写好了一个串行的 MapReduce 逻辑，提供了用于 word count 的 Map 和 Reduce 函数。我们先按照指南进行测试，看是否能输出正确结果，如果可以的话就开始着手写分布式 MapReduce 了（在 mr/ 目录下的三个文件中）。 MapReduce 原理通过阅读论文，我们可以知道 MapReduce 的作者是如何设计这个模型的： MapReduce原理模型 MapReduce 执行过程 用户程序中的 MapReduce 库（Client 端）首先将输入文件分割为 M 块，然后在集群上启动许多该程序的副本； 其中一个副本是 master，其余的是 worker。总共有 M 个 map 任务和 R 个 reduce 任务需要 master 挑选一个空闲的 worker 进行分配； 分配了 map 任务的 worker 读取相应输入 split 的内容。它从输入数据中解析键/值对，并将每对传递给用户定义的 map 函数。 map 函数生成的中间键/值对缓冲在内存中； 定期将键值对写入本地磁盘，并通过分区函数将其分为 R 个区域。这些键值在本地磁盘上的位置被传回 master，主节点负责将这些位置转发给 reduce 工作节点； 当一个 reduce worker 被通知位置后，通过 RPC 去读取 map workers 上的键值对，（并排序）； reduce worker 遍历所有的已排序键值对，将唯一的键和值集合传递给用户的 reduce 函数，函数输出到最终文件（最多 R 个）； 当所有的 reduce 完成任务后，master 唤醒（通知）用户程序。 整体流程还是比较清晰的，但是上述内容仅仅是一个逻辑模型，具体如何进行实现还是需要考虑很多问题的，并且与逻辑模型可能会有一些出入。 需要考虑的问题要真正实现 MapReduce，搞清以下几个问题很重要： 1 . Master 和 Worker 是何时初始化的？在初始化时各自接收了哪些参数？假设集群中每个机器运行一个 Master 或 Worker，这个概念并不是属于某个机器的，它只是这个机器上运行的一个进程。Master 和 Worker 是在用户向集群提交某次计算任务后才初始化的，用户提交的任务内容包括：输入文件路径、Map 函数、Reduce 函数。初始化的动作是由论文中提到的 “user program 调用的 MapReduce 库” 也就是整体框架进行的。 一般来说，MapReduce 系统的运行需要一个分布式文件系统的支持（如 HDFS），输入、输出文件均通过该系统的统一接口。 Master 初始化时接收的参数是：输入文件路径、分区数 R Worker 初始化时接收的参数是：Map 函数，Reduce 函数 2. Master 和 Worker 之间如何通信，是后者主动联系前者还是相反？大概有两种方式： 一种是 Worker 启动时向 Master 进行注册， Master 定时向 Worker 发送心跳确认其在线，并在有任务需要分配时主动通知 Worker。这也是论文的做法。 另一种是 Worker 不间断地向 Master 发送心跳，Master 接收到心跳时，将任务信息以回应的方式返回给 Worker。 我采用的是第二种做法，也是 Lab 倾向的做法。因为这样不论是 Master 和 Worker 实现起来要简洁一些，Worker 端不需要启动 RPC 服务器，性能也不输第一种。也是由于本 Lab 是运行在单机上的（为了测试方便），且采用了UNIX 域套接字进行进程间通信，所以在横向扩展 Worker 时第二种方式更加方便。 UNIX 域套接字用于在同一台计算机上运行的进程之间的通信。虽然因特网域套接字可用于同一目的，但 UNIX 域套接字的效率更高。UNIX 域套接字仅仅复制数据，它们并不执行协议处理，不需要添加或删除网络报头，无需计算校验和，不要产生顺序号，无需发送确认报文。 3. Master 如何区分不同的 Worker？Master 区分不同的 Worker 是为了记录任务的分配和执行情况，以便在出现异常时及时处理。 因为本次 Lab 是在一台机器上运行，所以不考虑 Worker 的网络地址。Master 可以通过 ID 来区分 Worker。在首次请求 Task 的时候让 Master 去赋予 Worker 一个全局唯一的 ID，这个 ID 的有效期直到 Job 运行结束。 4. Map 和 Reduce 任务分别有多少个？Map 任务的个数取决于输入文件的 split 个数，这个 split 的过程在论文中是由库函数进行的，但是一般来说在一个分布式文件系统中，它存储文件的方式本身就是 split 的形式，因此这一步视情况可以省略。在本 Lab 中，pg- 开头的每一个输入文件就是一个 split，它们的文件名被传给 Master 端。 Reduce 任务的个数小于等于分区数 R，是由用户指定的。Map 函数读取 split 文件生成大量的 KV 对，然后根据 hash(key) % R 的结果将中间键空间划分为 R 个片段，将每个片段的 KV 对输出到一个中间文件中去，每个中间文件都将被输入到一个 Reduce 函数。但是如果 KV 对的数量较少或者是数据较为倾斜，那么最终并不一定有 R 个文件，也就不一定有 R 个 Reduce 任务，所以 Reduce 任务的数量是不一定的，最大为 R。 但是在 Lab 代码中，官方将传递给 Master 的分区数 R 的变量命名为 nReduce，这其实是一种误导，所以我将它改成了 partition。 5. Master 如何在相应的阶段分配 Map 和 Reduce 任务？需要传递什么参数给 Worker？Worker 在完成一个任务以后需要返回什么结果给 Master？关于这几个问题，概述如下： 1. worker 循环调用 master 的 rpc 方法去获取任务（心跳），直到收到“结束”指令。 2. worker 收到 map 任务，从一个 split 文件读取数据并执行 map 函数，将缓冲对写入本地磁盘。 3. 当所有的 M 个 map 任务执行完毕后，master 开始分配 reduce 任务，仍然是 worker 主动获取。 4. worker 收到 reduce 任务，读取对应 worker 磁盘上的中间数据（这里就是本地的数据，因为在一台机器上运行），执行 reduce ()函数，将结果输出到一个最终 output 文件。 画了一张图： MapReduce数据流向示意图 其中，箭头是数据/信息流动的方向，蓝色代表 Map 阶段，红色代表 Reduce 阶段。整体的运行流程按照序号顺序进行。注意，同一个 Worker 可能先后完成多个 Map 和 Reduce 任务，图中的 Worker 编号只是一个示意。Master 相当于服务端，Worker 不断发送请求。在本 Lab 中，DFS 就是本地文件系统。 6. 如果某一个 Worker 掉线了怎么办？Master 端需要进行简单的容错，我采取的方式是：在 Master 分配给 Worker 一个任务后，异步计时等待（如 10 秒），等待结束后如果该 Worker 还没有返回计算结果，那么就认为该 Worker 掉线了，需要重新分配此任务。 MapReduce 具体实现代码细节较多，只记录较重要的部分。 Worker 实现每个 Worker 只管接受任务、执行计算、返回结果，不需要管别的，所以可以先从 Worker 写起。 每当 Worker 通过 RPC 向 Master 发送心跳请求的时候，会收到 4 种可能的回应： HEATBEAT：Master 的回应心跳，代表现在 Master 没有任务可分配，Worker 暂时空闲； MAPTASK：Master 向自己分配了一个 Map 任务，附带的信息有任务编号 X、输入文件路径、分区数 R； REDUCETASK：Master 向自己分配了一个 Reduce 任务，附带的信息有任务编号 Y、所有的中间文件路径； QUIT：计算 Job 的 Map 和 Reduce 阶段已全部完成，可以退出程序。 初始化代码如下： 12345678910111213141516171819202122232425262728293031323334func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) &#123; for &#123; request := Request&#123;&#125; reply := Reply&#123;&#125; request.WorkerId = workerId // 初始值为0 // 通过RPC向Master发送心跳 ok := call(&quot;Coordinator.HeartbeatHandler&quot;, &amp;request, &amp;reply) if !ok &#123; log.Printf(&quot;call Coordinator.HeartbeatHandler failed!\\n&quot;) return &#125; if workerId == 0 &#123; workerId = reply.WorkerId // 首次获取到Master分配的ID setLogFile() &#125; switch reply.Command &#123; case QUIT: // 结束 deleteIntermediates() // 删除产生的所有中间文件 log.Println(&quot;Quit.&quot;) return case HEARTBEAT: log.Println(&quot;Receive Heartbeat with master.&quot;) case MAPTASK: // 分配了map任务 log.Println(&quot;Got map task:&quot;, reply) doMapTask(reply.Task, mapf, reply.NReduce) case REDUCETASK: // 分配了reduce任务 log.Println(&quot;Got reduce task:&quot;, reply) doReduceTask(reply.Task, reducef) &#125; // 心跳间隔 time.Sleep(time.Second * time.Duration(HEARTBEAT_INTERVAL)) &#125;&#125; Worker 在初始化后 ID 为 0，Master 在遇见 ID 为 0 的请求后，会从 1 开始累加，向 Worker 分配 ID，Worker 需要保存这个 ID 直到程序结束，在随后的每次请求都要附上自己的 ID。 在收到 Map 任务时： 读取输入文件的全部内容； 调取用户的 map 函数，得到 KV 数组：kva := mapf(filename, string(content))； 遍历 KV 数组的每一个 KV： 计算当前 KV 应属分区号 Y：partition := ihash(kv.Key) % nReduce 如果不存在，新建中间文件：mr-X-Y 将该 KV 以 JSON 格式输出到中间文件 发送任务完成信息给 Master，内容包含所有中间文件的路径。 在收到 Reduce 任务时： 以 JSON 格式读取所有中间文件的内容到一个 KV 数组； 对该数组以 Key 进行排序； 如果不存在，新建临时结果文件：mr-out-Y-随机字符； 对于每一个 Key 和它对应的 Value 集合，调用用户的 reduce 函数：output := reducef(intermediate[i].Key, values) output 按格式输出到结果文件。 遍历完毕后，更改临时文件名为：mr-out-Y。 发送任务完成信息给 Master。 临时文件的目的是为了防止 reduce 执行到一半 worker 崩溃了，却留给用户任务已完成的假象。 Master 实现本次采用的 Master 端的模式是完全被动的，也就是不会主动去找 Worker 分配任务，这就需要通过某些设计，使得 Worker 的心跳请求到来后，判断当前是 Map 阶段还是 Reduce 阶段还是已经完成 Job 了。 首先看一下 Task 和 Master 的数据结构： 1234567891011121314151617181920type Task struct &#123; TaskType int // 任务类型，Map或Reduce TaskNo int // 任务编号，用于标识和文件命名 Files []string // 文件路径信息，map就是1个输入文件，reduce就是多个中间文件&#125;type Coordinator struct &#123; jobDone bool // Job是否完成 isReducing bool // 当前是否是Reduce阶段 nextWorkerId int // 下一个新Worker的ID nMap int // map任务个数 nReduce int // reduce任务个数 partition int // 分区数 unassignedMaps chan Task // 还未分配的map任务 unassignedReduces chan Task // 还未分配的reduce任务 assignedMaps map[int]bool // 已分配的map任务，TaskNo-&gt;是否完成 assignedReduces map[int]bool // 已分配的reduce任务，TaskNo-&gt;是否完成 working map[int]*Task // 正在工作的worker记录，ID-&gt;Task intermediates map[int][]string // 中间文件名集合，reduceNo-&gt;files mu sync.Mutex // 互斥锁，保证并发安全&#125; 我采用的是 Go 的 Buffered Channel 进行任务分配。在 Master 初始化的时候，先将每一个输入 split 文件创建一个 MapTask，并压入未分配 Map 任务的队列： 1234// 初始化map任务for index, file := range files &#123; c.unassignedMaps &lt;- Task&#123;MAPTASK, index, []string&#123;file&#125;&#125;&#125; 然后就是应对 Worker 的 RPC 请求处理函数了，总共有三个： HeartbeatHandler，处理心跳请求——分配任务 MapFinishedHandler，处理 Map 任务完成信息 ReduceFinishedHandler，处理 Reduce 任务完成信息 代码如下： 分配任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (c *Coordinator) HeartbeatHandler(request *Request, reply *Reply) error &#123; log.Println(&quot;Receive heartbeat from worker:&quot;, request.WorkerId) c.mu.Lock() defer c.mu.Unlock() workerId := request.WorkerId if workerId == 0 &#123; workerId = c.nextWorkerId c.nextWorkerId++ &#125; reply.WorkerId = workerId workingTask, exist := c.working[workerId] if exist &#123; // master的记录中该worker还在工作 // 说明之前的任务可能失败了，需要将task重新入队 c.unassignTask(workingTask) delete(c.working, workerId) &#125; select &#123; case mapTask := &lt;-c.unassignedMaps: // 分配map reply.Task = mapTask reply.Command = MAPTASK reply.NReduce = c.partition c.assignedMaps[mapTask.TaskNo] = false c.working[workerId] = &amp;mapTask go c.checkStalled(workerId, &amp;mapTask) default: // map全部分配了 if allTaskFinished(c.assignedMaps) &#123; // map全部执行完了 select &#123; case reduceTask := &lt;-c.unassignedReduces: // 分配reduce reply.Task = reduceTask reply.Command = REDUCETASK c.assignedReduces[reduceTask.TaskNo] = false c.working[workerId] = &amp;reduceTask go c.checkStalled(workerId, &amp;reduceTask) default: // reduce分配完了 - 也有可能没完成初始化，所以需要isReducing判断 if c.isReducing &amp;&amp; allTaskFinished(c.assignedReduces) &#123; // reduce全部执行完了 reply.Command = QUIT // 结束任务 &#125; &#125; &#125; &#125; return nil&#125; Map 任务完成 123456789101112131415161718192021222324252627282930313233func (c *Coordinator) MapFinishedHandler(request *Request, reply *Reply) error &#123; c.mu.Lock() defer c.mu.Unlock() log.Println(&quot;Receive completion of map task from worker:&quot;, request.WorkerId) mapTask, ok := c.working[request.WorkerId] if ok &#123; c.assignedMaps[mapTask.TaskNo] = true delete(c.working, request.WorkerId) // 收集worker产生的中间文件名 for rNo, file := range request.Intermediates &#123; arr, ok := c.intermediates[rNo] if !ok &#123; arr = []string&#123;file&#125; &#125; else &#123; arr = append(arr, file) // 累加 &#125; c.intermediates[rNo] = arr &#125; if len(c.assignedMaps) == c.nMap &amp;&amp; allTaskFinished(c.assignedMaps) &#123; // map全部分配完，并且全部完成 // 初始化reduce任务 log.Println(&quot;start reduce&quot;) c.nReduce = len(c.intermediates) // 这才是真正的reduce数量 for rNo, files := range c.intermediates &#123; // log.Println(rNo, files) c.unassignedReduces &lt;- Task&#123;REDUCETASK, rNo, files&#125; &#125; c.isReducing = true &#125; &#125; else &#123; log.Println(&quot;Worker&quot;, request.WorkerId, &quot;&#x27;s result of map task was discarded.&quot;) &#125; return nil&#125; Reduce 任务完成 1234567891011121314151617func (c *Coordinator) ReduceFinishedHandler(request *Request, reply *Reply) error &#123; c.mu.Lock() defer c.mu.Unlock() log.Println(&quot;Receive completion of reduce task from worker:&quot;, request.WorkerId) reduceTask, ok := c.working[request.WorkerId] if ok &#123; c.assignedReduces[reduceTask.TaskNo] = true delete(c.working, request.WorkerId) if len(c.assignedReduces) == c.nReduce &amp;&amp; allTaskFinished(c.assignedReduces) &#123; // reduce全部分配完，并且全部完成 log.Println(&quot;=== Job done! ===&quot;) c.jobDone = true &#125; &#125; else &#123; log.Println(&quot;Worker&quot;, request.WorkerId, &quot;&#x27;s result of reduce task was discarded.&quot;) &#125; return nil&#125; 在每次分配完任务后，都需要另起协程判断 Worker 是否阻塞（掉线）了： 123456789101112func (c *Coordinator) checkStalled(workerId int, task *Task) &#123; time.Sleep(time.Second * time.Duration(WAIT_WORKER)) // 等待一段时间 c.mu.Lock() defer c.mu.Unlock() currTask, ok := c.working[workerId] // 如果还能从working中获取到对应ID的任务 if ok &amp;&amp; currTask == task &#123; // 说明worker仍在执行之前的任务，需要踢出 c.unassignTask(task) // 将task从已分配中删除，加入未分配队列 // 删除工作记录 delete(c.working, workerId) log.Println(&quot;worker:&quot;, workerId, &quot;was kicked out.&quot;) &#125;&#125; 测试全部通过：","categories":[],"tags":[{"name":"go","slug":"go","permalink":"https://zyrate.github.io/tags/go/"},{"name":"mit6824","slug":"mit6824","permalink":"https://zyrate.github.io/tags/mit6824/"},{"name":"分布式","slug":"分布式","permalink":"https://zyrate.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"MIT6.830 SimpleDB 实现笔记 Lab 6","slug":"830 Lab 6","date":"2023-11-14T16:00:00.000Z","updated":"2024-03-12T09:59:28.272Z","comments":true,"path":"2023/11/15/830 Lab 6/","link":"","permalink":"https://zyrate.github.io/2023/11/15/830%20Lab%206/","excerpt":"","text":"Lab 6 实现 SimpleDB 的基于日志系统的回滚（rollback）和恢复（recover）功能。 WAL 机制首先，SimpleDB 要实现的是预写式日志（Write-ahead logging, WAL），也就是所有修改在生效之前都要先写入 log 文件中，写入的内容包括 redo 和 undo 信息，分别保证事务的持久性和原子性。 在 SimpleDB 中，日志的单位和锁一样是页面，每个页面都可以通过 setBeforeImage 方法来设置 oldData，也就是每次 flush 到磁盘前页面还未变动时的旧数据（是上一次 flush 的时候保存的）。日志系统会在每次 flush 脏页的时候把 beforeImage 和 afterImage 写入日志文件，分别代表旧数据和新数据。这样在需要 redo 的时候，就把 afterImage 写入磁盘；需要 undo 的时候，就把 beforeImage 写入磁盘。 Lab 6 只要求实现 LogFile 类中的 rollback 和 recover 方法： 前者用在事务 abort 的时候，需要撤销 (undo) 该事务的所有操作，回滚数据库到之前的状态； 后者用在发生崩溃 crash 的时候，需要撤销 (undo) 所有未提交事务的所有操作、重做 (redo) 所有已提交事务的所有操作，恢复数据库到正常状态。 缓冲区管理策略数据库的缓冲区管理策略有两类四种，分别是： steal 策略允许从页面缓存逐出“脏页”。此时磁盘上可能包含 uncommitted 的数据，因此系统需要记录 undo log，以在事务 abort 时进行回滚（rollback）。 no-steal 策略不允许从页面缓存逐出“脏页”。表示磁盘上不会存在 uncommitted 数据，因此无需回滚操作，也就无需记录 undo log。 force 策略事务在 committed 的时候必须将所有更新立刻持久化到磁盘，这样的话不需要 redo log，因为只要日志中存在 commit 记录就说明磁盘已经更新了全部数据。但是这样会导致磁盘发生很多小的写操作（更可能是随机写）。 no-force 策略事务在 committed 之后可以不立即持久化到磁盘，这样可以缓存很多的脏页批量持久化到磁盘，这样可以降低磁盘操作次数（提升顺序写），但是如果 committed 之后发生crash，那么此时已经提交的事务数据将会丢失（因为还没有持久化到磁盘），因此系统需要记录 redo log，在系统重启时候进行回复（recover）操作。 在 SimpleDB 中, 之前的 Lab 要求实现的是 no-steal 和 force 策略，但是这种策略的效率不高。所以在本次 Lab 的 LogTest 中，它会时不时的打破 no-steal 策略，也就是通过随时调用 flushAllPages() 让磁盘上存在未提交的数据，测试 abort 后的回滚操作。同时也默认 no-force 的存在（虽然实际不是），以测试 crash 后的恢复操作。所以我们 redo 和 undo 都需要实现。 日志文件结构Log File 中一条记录的格式是：&lt;RECORD_TYPE:int | TID:long | content | start:long&gt; 其中 RECORD_TYPE 指记录的类型，TID 指事务的标识，content 在不同的类型中表示不同内容， start 指此条记录开始位置的偏移量。 RECORD_TYPE 总共有 5 种表示不同的行为： BEGIN, 事务开始 UPDATE, 事务对页面进行 UPDATE 操作 COMMIT, 事务提交 ABORT, 事务中断 CHECKPOINT, 检查点 在运行过程中各类记录被不停地追加到 Log File 里面。 由于多个事务之间时并行执行的，所以日志文件里不同事务对不同页面的各项操作是混合交叉在一起的。 BEGIN、COMMIT 和 ABORT 这三种记录的 content 位置是空的，不存储数据；而 UPDATE 存储的是序列化后的 beforeImage 和 afterImage；CHECKPOINT 存储的首先是一个 INT 类型代表当前活跃事务（未提交）的数量，后面跟的是每个活跃事务的 TID 和 BEGIN 记录的位置 offset（都是 Long 类型）。 检查点是为了加快恢复过程的速度。如果没有检查点，那么系统在宕机重启后需要从头对 Log File 进行顺序访问，依次找到所有未提交和已提交的事务进行 undo 和 redo 操作，费时费力。而检查点机制要求在向 Log File 中添加 CHECKPOINT 的时候，将缓冲区中所有的脏页刷新到磁盘，也就代表着在检查点之前提交了的事务无需在重启后执行恢复操作，因为磁盘已经拥有这些事务更新后的数据。我们只需从检查点之后顺序访问 Log File 即可。 另外需要注意的是，检查点会记录那个时刻还未提交的所有事务 ID，这些事务并不能保证宕机后的原子性和持久性，因此也需要对这些事务进行恢复操作。 Rollback 实现rollback 方法在事务被 abort 的时候调用，此时该事务对所有页面产生的所有修改都应该失效，也就是说需要将所有相关页面的 beforeImage（旧数据）恢复到磁盘上（undo）。 123456789101112131415161718192021222324252627282930313233343536373839public void rollback(TransactionId tid)&#123;//省略synchronized结构 preAppend(); // 找到该事务在file中的第一个记录的偏移量 long offset = tidToFirstLogRecord.get(tid.getId()); raf.seek(offset); // 顺序访问直到文件末尾 while (true) &#123; try &#123; int type = raf.readInt(); // 记录类型 long record_tid = raf.readLong(); // TID switch (type) &#123; case UPDATE_RECORD: // 更新记录 Page before = readPageData(raf); // 旧数据 Page after = readPageData(raf); // 新数据 if(record_tid == tid.getId())&#123; // 先把此页面从缓存中去除 Database.getBufferPool().discardPage(before.getId()); // 然后把旧数据写入Table文件 Database.getCatalog().getDatabaseFile(before.getId().getTableId()).writePage(before); &#125; break; case CHECKPOINT_RECORD: // 跳过所有检查点记录 int numXactions = raf.readInt(); while (numXactions-- &gt; 0) &#123; long xid = raf.readLong(); long xoffset = raf.readLong(); &#125; break; &#125; raf.readLong(); // 跳过start指针 &#125; catch (EOFException e) &#123; break; &#125; &#125;&#125; Recover 实现recover 方法在数据库 crash 重启后调用，需要将检查点（如果有的话）中及其之后的所有事务进行恢复操作，未提交的 undo，已提交的 redo。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106public void recover() throws IOException &#123;// 省略synchronized结构 recoveryUndecided = false; /* redo就是写入afterimage，undo就是写入beforeimage */ // 已提交的事务ID集合 Set&lt;Long&gt; commitedIds = new HashSet&lt;&gt;(); // 检查点存储的活跃事务集合 Map&lt;Long, Long&gt; activeTxns = new HashMap&lt;&gt;(); // 从检查点往后所有事务的集合（所有的旧页面和新页面） Map&lt;Long, List&lt;Page&gt;&gt; beforePages = new HashMap&lt;&gt;(); Map&lt;Long, List&lt;Page&gt;&gt; afterPages = new HashMap&lt;&gt;(); long cpOffset = raf.readLong(); // 检查点位置 if(cpOffset != -1)&#123; raf.seek(cpOffset); // 如果有检查点，直接从此处开始 &#125; // 顺序访问直到文件末尾 while (true) &#123; try &#123; int type = raf.readInt(); // 记录类型 long record_tid = raf.readLong(); // TID switch (type) &#123; case UPDATE_RECORD: Page before = readPageData(raf); // 旧数据 Page after = readPageData(raf); // 新数据 beforePages.computeIfAbsent(record_tid, k-&gt;new ArrayList&lt;&gt;()).add(before); afterPages.computeIfAbsent(record_tid, k-&gt;new ArrayList&lt;&gt;()).add(after); break; case CHECKPOINT_RECORD: int numXactions = raf.readInt(); while (numXactions-- &gt; 0) &#123; long xid = raf.readLong(); long xoffset = raf.readLong(); activeTxns.put(xid, xoffset); // 记录活跃事务 &#125; break; case COMMIT_RECORD: commitedIds.add(record_tid); // 记录已提交事务 break; &#125; raf.readLong(); // 跳过start指针 &#125; catch (EOFException e) &#123; break; &#125; &#125; /* 注意undo和redo的顺序不能乱，否则redo被undo覆盖 */ // undo未commit的 for(Long record_id : beforePages.keySet())&#123; if(!commitedIds.contains(record_id))&#123; List&lt;Page&gt; befores = beforePages.getOrDefault(record_id, new ArrayList&lt;&gt;()); for(Page page : befores)&#123; Database.getCatalog().getDatabaseFile(page.getId().getTableId()).writePage(page); &#125; &#125; &#125; // redo已经commit的 for(Long record_tid : commitedIds)&#123; List&lt;Page&gt; afters = afterPages.getOrDefault(record_tid, new ArrayList&lt;&gt;()); for(Page page : afters)&#123; Database.getCatalog().getDatabaseFile(page.getId().getTableId()).writePage(page); &#125; &#125; // 处理在checkpoint之前开始但是在checkpoint还未提交的事务 for(Map.Entry&lt;Long,Long&gt; entry : activeTxns.entrySet())&#123; long active_id = entry.getKey(); long active_offset = entry.getValue(); boolean commited = commitedIds.contains(active_id); raf.seek(active_offset); // 代码与上文类似 while (true) &#123; try &#123; int type = raf.readInt(); long record_tid = raf.readLong(); switch (type) &#123; case UPDATE_RECORD: Page before = readPageData(raf); Page after = readPageData(raf); if(commited)&#123; // redo Database.getCatalog().getDatabaseFile(after.getId().getTableId()).writePage(after); &#125;else&#123; // undo Database.getCatalog().getDatabaseFile(before.getId().getTableId()).writePage(before); &#125; break; case CHECKPOINT_RECORD: int numXactions = raf.readInt(); while (numXactions-- &gt; 0) &#123; long xid = raf.readLong(); long xoffset = raf.readLong(); &#125; break; &#125; raf.readLong(); &#125; catch (EOFException e) &#123; break; &#125; &#125; &#125;&#125; 需要注意 undo 和 redo 的顺序不能颠倒，否则会出现数据覆盖问题。 image.png Lab 仓库地址：zyrate/simple-db-hw-2021 (github.com)","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"mit6830","slug":"mit6830","permalink":"https://zyrate.github.io/tags/mit6830/"}]},{"title":"MIT6.830 SimpleDB 实现笔记 Lab 5","slug":"830 Lab 5","date":"2023-11-06T16:00:00.000Z","updated":"2024-03-12T09:59:35.926Z","comments":true,"path":"2023/11/07/830 Lab 5/","link":"","permalink":"https://zyrate.github.io/2023/11/07/830%20Lab%205/","excerpt":"","text":"Lab 5 要求实现 SimpleDB 的 B+树索引存储机制。整体的 B+树相关代码还是较为复杂的，但是 SimpleDB 帮我们写了大部分结构性的代码，让我们去完成较重要的功能性代码。但是所有的代码都需要理解透彻。 B+树索引结构首先，在 InnoDB 引擎中，B+树索引分为主索引和辅助索引，主索引的叶子结点记录着完整的数据，而辅助索引的叶子结点只记录着主键的值，在查找时需要先插找到主键值，再到主索引中进行查找，相当于二级索引机制。但在 SimpleDB 中，只要求实现主索引。 一个关于索引的定义：索引就是一个表属性子集的副本，为了通过这些属性更高效的访问数据而进行了一定的组织和排序，数据库需要确保表和索引是逻辑同步的。这里可以理解成，B+树的非叶结点就是上述的“属性子集的副本”，是真正的索引结构，而叶结点并不是副本，是表的属性数据集合本身。 也就是说，在 SimpleDB 中一个表可以用 B+树 File 形式存储，也可以用 HeapFile 形式存储，这两者之前是平行关系，取决于用户的选择，它们拥有相同的存储方式和顶层抽象。 对于 HeapFile，它继承自 DbFile，内部存储 HeapPage，通过 BufferPool 访问； 对于 BTreeFile，它同样继承自 DbFile，内部存储 BTreePage，也通过 BufferPool 访问，缓存和锁管理机制是通用的。 但是由于 B+树结构复杂，因此有多种不同类型的 BTreePage，分别是：BTreeRootPtrPage、BTreeHeaderPage、BTreeInternalPage、BTreeLeafPage。但其实，与 B+树的逻辑结构相关的只有最后两种，分别对应内部结点和叶子结点。B+树逻辑结构 而 BTreeHeaderPage 的作用是记录当前 BTreeFile 中还有没有空页（由于从 B+树删除结点并不会在物理层面也删除，有点像静态哈希表），如果有空页那么新建结点的时候就可以复用该页，如果没有就新建一个空页到 BTreeFile 中去。 BTreeRootPtrPage 的作用是记录根结点（internal 或 leaf）的在 File 中的位置，因为在进行一系列的插入、删除操作后，File 中的 Page 是无序存储的，B+树逻辑结构靠的是地址链接，所以 File 的第一个 Page 不一定是根结点，所以需要记录。BTreeRootPtrPage 中有一个静态方法 getId()，可以看到新建了一个 pageNo=0 的 PageId，所以这个页面永远储存在 File 的第一个位置。 在做此 Lab 的时候，不用想着在一开始搞懂所有类的所有方法，容易没有头绪。就从 Lab 要求补充的代码处入手，需要用什么结构、方法、接口就去了解什么，很容易就上手了。 BTreeLeafPage 存储着该叶子结点上的所有 Tuple 和左右兄弟指针，调用 iterator() 可以依次（正向或逆向）遍历 Tuple。 BTreeInternalPage 存储着所有的key值和指向孩子节点的指针，调用 iterator() 可以一次（正向或逆向）遍历 BTreeEntry，其中包含“key、左指针、右指针”。这个实体只是为了传递数据，修改里面的值并不会对结点本身造成影响，如果想更新结点数据，需要调用 updateEntry() 方法。 所有的“指针”都是指页面的 PageNo，类型是 int。 B+树的查找核心思想： 对于一个目标值 f，从根结点开始查找，遍历该结点所有的 key 值，如果 f&lt;=key ，那么进入这个 key 的左孩子递归查找；如果遍历到了最后一个 key 仍不符合条件，那么进入右孩子递归查找。直到遇到叶子结点，直接返回此结点，因为它一定包含 f（或 f 不在 B+树中）。 123456789101112131415161718192021222324private BTreeLeafPage findLeafPage(TransactionId tid, Map&lt;PageId, Page&gt; dirtypages, BTreePageId pid, Permissions perm, Field f) throws DbException, TransactionAbortedException &#123; if(pid.pgcateg() == BTreePageId.INTERNAL)&#123; // 非叶结点 // 从BufferPool拿到指定pid的非叶页面 BTreeInternalPage inPage = (BTreeInternalPage) getPage(tid, dirtypages, pid, Permissions.READ_ONLY); Iterator&lt;BTreeEntry&gt; iterator = inPage.iterator(); // 遍历该页面的所有key，和目标f作比较（见B+树的查找） while(iterator.hasNext())&#123; BTreeEntry entry = iterator.next(); if(f==null||f.compare(Op.LESS_THAN_OR_EQ, entry.getKey()))&#123; // 目标值为null或小于等于key值，进入左孩子递归 return findLeafPage(tid, dirtypages, entry.getLeftChild(), perm, f); &#125; if(!iterator.hasNext())&#123; // 遍历到最后一个，进入右孩子递归 return findLeafPage(tid, dirtypages, entry.getRightChild(), perm, f); &#125; &#125; &#125;else if(pid.pgcateg() == BTreePageId.LEAF)&#123; // 叶子结点，直接返回指定pid的叶子页面 return (BTreeLeafPage) getPage(tid, dirtypages, pid, perm); &#125; return null; &#125; getPage() 方法里面调用了 BufferPool 的 getPage 方法，保证了锁机制的正常执行，另外它接受并维护一个 dirtypages 映射，每当访问类型是 READ_WRITE 的时候就将该页面设为 dirty，以便记录脏页集合。 B+树的插入利用 B+树的查找代码，找到需要插入的元组应该在的叶子结点，直接插入。如果该结点已经满了，没有空位可以插入了，则需要进行分裂，分裂的过程可能会递归向上。 叶子结点的分裂： 找到中间 Tuple 位置，将该 Tuple 及其右侧的所有 Tuple 依次移动到一个新的 LeafPage 上，之后将这个中间 Tuple 的 key 值插入到父结点中，最后更新所有相关的指针。 叶结点的分裂 内部结点的分裂： 找到中间 key 位置，将该 key 右侧的所有 key 依次移动到一个新的 InternalPage 上，之后将这个 key 从原 Page 删除，插入到父节点中。 内部结点的分裂 两类分裂的区别在于，叶子结点分裂需要将中间 Tuple 的 key 复制到父结点中，而内部结点的分裂是将中间 key上移到父结点中，并且需要更新新页面所有子页面的父指针（因为这些子页面的父指针都指向原来的页面）。 在分裂的时候，需要向父页面插入值，此时父页面也可能会出现无空位的情况，递归处理即可。 叶结点的分裂代码： 1234567891011121314151617181920212223242526272829303132333435363738394041public BTreeLeafPage splitLeafPage(TransactionId tid, Map&lt;PageId, Page&gt; dirtypages, BTreeLeafPage page, Field field) throws DbException, IOException, TransactionAbortedException &#123; // 分裂位置 int splitFrom = page.getNumTuples() / 2; // 新叶子结点 BTreeLeafPage newLeaf = (BTreeLeafPage) getEmptyPage(tid, dirtypages, BTreePageId.LEAF); Iterator&lt;Tuple&gt; iterator = page.reverseIterator(); Field middleKey = null; // 中间Key // 逆序遍历旧叶子结点进行元组移动 for(int i=page.getNumTuples()-1; iterator.hasNext() &amp;&amp; i&gt;=splitFrom; i--)&#123; Tuple t = iterator.next(); page.deleteTuple(t); newLeaf.insertTuple(t); if(i == splitFrom)&#123; middleKey = t.getField(keyField); &#125; &#125; // 左右兄弟连接 BTreePageId rightSiblingId = page.getRightSiblingId(); newLeaf.setLeftSiblingId(page.getId()); newLeaf.setRightSiblingId(page.getRightSiblingId()); page.setRightSiblingId(newLeaf.getId()); if(rightSiblingId != null)&#123; // 不要忘了这一步的指针更新 BTreeLeafPage rightSibling = (BTreeLeafPage) getPage(tid, dirtypages, rightSiblingId, Permissions.READ_WRITE); rightSibling.setLeftSiblingId(newLeaf.getId()); &#125; /* key上移 */ BTreeEntry bTreeEntry = new BTreeEntry(middleKey, page.getId(), newLeaf.getId()); // 这个提供的方法里面有向上递归的部分 BTreeInternalPage parent = getParentWithEmptySlots(tid, dirtypages, page.getParentId(), field); parent.insertEntry(bTreeEntry); page.setParentId(parent.getId()); newLeaf.setParentId(parent.getId()); // 要插入的Key如果小于middle key，返回左（旧）叶子 if(field.compare(Op.LESS_THAN, middleKey))&#123; return page; &#125; // 否则返回右（新）叶子 return newLeaf; &#125; 内部结点分裂代码： 1234567891011121314151617181920212223242526272829303132public BTreeInternalPage splitInternalPage(TransactionId tid, Map&lt;PageId, Page&gt; dirtypages, BTreeInternalPage page, Field field) throws DbException, IOException, TransactionAbortedException &#123; // 新非叶结点 BTreeInternalPage newInternal = (BTreeInternalPage) getEmptyPage(tid, dirtypages, BTreePageId.INTERNAL); Iterator&lt;BTreeEntry&gt; iterator = page.reverseIterator(); BTreeEntry middleEntry = null; // 中间entry // 遍历旧非叶结点进行元组移动 for(int i=page.getNumEntries()-1; iterator.hasNext() &amp;&amp; i&gt;=splitFrom; i--)&#123; BTreeEntry e = iterator.next(); page.deleteKeyAndRightChild(e); if(i == splitFrom)&#123; middleEntry = e; break; &#125; newInternal.insertEntry(e); &#125; // 这个提供的方法里面有向上递归的部分 BTreeInternalPage parent = getParentWithEmptySlots(tid, dirtypages, page.getParentId(), field); middleEntry.setLeftChild(page.getId()); middleEntry.setRightChild(newInternal.getId()); parent.insertEntry(middleEntry); page.setParentId(parent.getId()); newInternal.setParentId(parent.getId()); // 更新新结点所有子页面的父指针 updateParentPointers(tid, dirtypages, newInternal); // 要插入的Key如果小于middle key，返回左（旧）非叶 if(field.compare(Op.LESS_THAN, middleEntry.getKey()))&#123; return page; &#125; // 否则返回右（新）非叶 return newInternal; &#125; B+树的删除B+树的删除有三种情况： 找到对应的叶子结点，删除 Tuple； 如果删除后该叶子结点的元组数小于 half full，且兄弟结点大于 half full，那么就从兄弟结点STEAL一些元组，以保持平衡； 如果兄弟结点已经是 half full 了，那么就合并这两个结点。两类结点的STEAL过程 在合并叶子结点的时候，会从父结点删除一个 key，因此需要递归向上判断。 注意在 STEAL 的时候，叶子结点是在转移完兄弟结点的 Tuple 后，根据情况把对应的 key 值复制到父结点，而内部结点则是移动到父结点。 在合并的时候，叶子结点是合并以后，父结点的对应 key 直接删除；而内部结点是将父结点对应 key 下移到对应位置。两类结点的合并过程 叶子结点的 STEAL 操作代码： 123456789101112131415161718192021public void stealFromLeafPage(BTreeLeafPage page, BTreeLeafPage sibling, BTreeInternalPage parent, BTreeEntry entry, boolean isRightSibling) throws DbException &#123; // 需要偷取的个数 int numToMove = (sibling.getNumTuples() - halfFull) / 2; Iterator&lt;Tuple&gt; iterator; if(isRightSibling)&#123; // 从左右不同结点偷取的顺序相反 iterator = sibling.iterator(); &#125;else&#123; iterator = sibling.reverseIterator(); &#125; // steal for(int i=0; i&lt;numToMove &amp;&amp; iterator.hasNext(); i++)&#123; Tuple t = iterator.next(); sibling.deleteTuple(t); page.insertTuple(t); &#125; // 如果从左边偷的，本页第一个key上替；如果从右边偷的，右边第一个key上替 Field key = isRightSibling ? sibling.iterator().next().getField(keyField) : page.iterator().next().getField(keyField); entry.setKey(key); parent.updateEntry(entry); &#125; 从左侧内部结点 STEAL 操作代码（右侧同理）： 1234567891011121314151617181920212223242526public void stealFromLeftInternalPage(TransactionId tid, Map&lt;PageId, Page&gt; dirtypages, BTreeInternalPage page, BTreeInternalPage leftSibling, BTreeInternalPage parent, BTreeEntry parentEntry) throws DbException, TransactionAbortedException &#123; // 偷取个数 int numToMove = (leftSibling.getNumEntries() - halfFull) / 2; Iterator&lt;BTreeEntry&gt; iterator = leftSibling.reverseIterator(); RecordId parentRecordId = parentEntry.getRecordId(); for(int i=0; i&lt;numToMove &amp;&amp; iterator.hasNext(); i++)&#123; BTreeEntry e = iterator.next(); leftSibling.deleteKeyAndRightChild(e); if(i == 0)&#123; // 开始的时候首先把父Entry旋转下来 parentEntry.setLeftChild(e.getRightChild()); parentEntry.setRightChild(page.iterator().next().getLeftChild()); page.insertEntry(parentEntry); &#125;else if(i == numToMove-1)&#123; // 到最后一个再把Entry旋转到父Entry的位置 e.setLeftChild(leftSibling.getId()); e.setRightChild(page.getId()); e.setRecordId(parentRecordId); // 必须要提前记录父Entry的RecordId，不然无法更新 parent.updateEntry(e); break; &#125; page.insertEntry(e); &#125; // 更新两个非叶Page的子Page的父指针 updateParentPointers(tid, dirtypages, page); updateParentPointers(tid, dirtypages, leftSibling); &#125; Merge 叶子结点代码： 1234567891011121314151617181920public void mergeLeafPages(TransactionId tid, Map&lt;PageId, Page&gt; dirtypages, BTreeLeafPage leftPage, BTreeLeafPage rightPage, BTreeInternalPage parent, BTreeEntry parentEntry) throws DbException, IOException, TransactionAbortedException &#123; while(iterator.hasNext())&#123; Tuple t = iterator.next(); rightPage.deleteTuple(t); leftPage.insertTuple(t); &#125; // 更新指针 BTreePageId rightSibling = rightPage.getRightSiblingId(); leftPage.setRightSiblingId(rightSibling); if(rightSibling != null)&#123; BTreeLeafPage rightSiblingPage = (BTreeLeafPage) getPage(tid, dirtypages, rightSibling, Permissions.READ_WRITE); rightSiblingPage.setLeftSiblingId(leftPage.getId()); &#125; // 删除父页面的Entry - 包含向上递归 deleteParentEntry(tid, dirtypages, leftPage, parent, parentEntry); // 清空右页面 setEmptyPage(tid, dirtypages, rightPage.getId().getPageNumber()); &#125; Merge 内部结点代码： 12345678910111213141516171819public void mergeInternalPages(TransactionId tid, Map&lt;PageId, Page&gt; dirtypages, BTreeInternalPage leftPage, BTreeInternalPage rightPage, BTreeInternalPage parent, BTreeEntry parentEntry) throws DbException, IOException, TransactionAbortedException &#123; // 删除父页面的Entry - 包含向上递归 deleteParentEntry(tid, dirtypages, leftPage, parent, parentEntry); // 父Entry先插下来 parentEntry.setLeftChild(leftPage.reverseIterator().next().getRightChild()); parentEntry.setRightChild(rightPage.iterator().next().getLeftChild()); leftPage.insertEntry(parentEntry); // 右全部移动到左 while(iterator.hasNext())&#123; BTreeEntry e = iterator.next(); rightPage.deleteKeyAndLeftChild(e); leftPage.insertEntry(e); &#125; // 清空右页面 setEmptyPage(tid, dirtypages, rightPage.getId().getPageNumber()); // 更新左页面子页面的父指针 updateParentPointers(tid, dirtypages, leftPage); &#125; Bug 记录 注意左子树的 key 小于等于当前结点的 key，如果漏掉等于的话在查找重复值的情况会出错。 不论是对 Tuple 还是 Entry，都要先 delete 再 insert，因为 insert 操作会更新 RecordId，如果此时 delete 的话会报错。 全部做完之后发现部分代码无法通过 checkRep()。 一个 BUG 是在 MergeLeafPages 的时候忘了更新右页面的左指针（需要 getPage） 另一个 BUG 是在 stealFromRightInternalPage 的时候没想到只移动一个 Entry 的情况，这个时候两个条件不能是 if-else if 的关系，而应该是并列的。 还有一个 BUG 是在 splitLeafPage 的时候同样忘记更新右页面的左指针。 在执行 BTreeTest 的时候，虽然通过了，但一直在报 ConcurrentModificationException 错（从死锁检测抛出），推测是 adjList 里面的 List 不是线程安全的，归根结底是 PageLock 的 holds 没有采用 CopyOnWriteArrayList，更改后消除报错。 image.png Lab 仓库地址：zyrate/simple-db-hw-2021 (github.com)","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"mit6830","slug":"mit6830","permalink":"https://zyrate.github.io/tags/mit6830/"}]},{"title":"MIT6.830 SimpleDB 实现笔记 Lab 4","slug":"830 Lab 4","date":"2023-10-24T16:00:00.000Z","updated":"2024-03-12T09:59:40.563Z","comments":true,"path":"2023/10/25/830 Lab 4/","link":"","permalink":"https://zyrate.github.io/2023/10/25/830%20Lab%204/","excerpt":"","text":"Lab4 是实现 SimpleDB 的并发事务系统，跟前面的内容相比较为复杂。 一般来说数据库的事务需要满足 ACID 特性，即原子性、一致性、隔离性、持久性。原子性就是该事务的所有操作要么全部完成，要么全部取消，要求通过下面的操作保证： 不从页面缓存中逐出“脏页”（被某个事务更新的页面）。——NO STEAL 规则 在事务正确提交时，强制刷新所有脏页到磁盘。 隔离性就是同时执行的多个事务不会相互干扰，通过将要实现的锁机制保证。一致性在 SimpleDB 中没有强调，持久性应该在 Lab6 的恢复功能上体现。 SimpleDB 事务并发控制实现锁机制在数据库中锁定对象可以是表、页面、元组、属性等，SimpleDB 规定的锁定粒度是页面（Page）。可供事务获取的锁类型有两种：共享锁和排他锁，规则如下。 事务在读取页面之前，必须具有共享锁； 事务在修改页面之前，必须具有排他锁； 多个事务可以在一个页面上具有共享锁； 只有一个事务可以在一个对象有排他锁。 在该规则下，如果一个事务在请求页面的时候，无法获取该页面的锁，就必须被阻塞，以等待锁资源被其他事务释放留给自己去竞争。特别的是，如果一个事物在申请排他锁时，如果已经持有了该页面的共享锁且是唯一一个持有者，那么可将此共享锁升级为排他锁（锁升级）。 两阶段锁（2PL）考虑两个事务按照上述锁机制正常执行，有可能发生下图的情况：（X 排他，S 共享）T1 和 T2 都正常提交了，但是 T1 对于页面 A 发生了“不可重复读”现象，即在同一个事务先后两次读到的数据有可能不一样（被别的事务如 T2 修改了）。 解决这个问题的办法就是实现两阶段锁协议（2PL）。2PL 的两个阶段分别是扩展阶段（Growing） 和 收缩阶段（Shrinking），在扩展阶段事务只能获取锁，在收缩阶段事务只能释放锁。 两阶段锁协议本身足以保证冲突可串行性，但它可能会导致级联中止（Cascading aborts） 问题，即一个事务的中止可能导致其他多个事务也一起中止。这是因为在 2PL 中一个事务可能基于另一个事务尚未提交的数据进行操作，如果那个事务被中止，就会发生级联中止。如下图：为了避免这种情况，需要实现严格两阶段锁协议（Strict 2PL），即一个事务只能在它提交或中止时释放所有锁。SimpleDB 要求实现 Strict 2PL 协议。这其实简化了操作，因为在赋予事务锁的时候不用考虑什么时候执行完了操作该释放，而是通通等到最后 commit 时释放。 页面级锁机制实现刚开始时看到需要实现读写锁，自然会想到 JUC 中的 ReentrantReadWriteLock 类，然而该类是负责线程同步的，一个事务可以有多个线程，所以他们不在同一粒度。另外只用 Java 提供的这些类并不能很好地实现 2PL 协议，也并不契合事务并发场景。因此我们需要自己实现事务的读写锁机制，但是类库中的一些思想可以借鉴。 我们定义一个 LockManger 来负责维护事务和锁的状态，在 BufferPool 中事务只需调用相应的方法来获取和释放锁就行了。可以想到 LockManger 需要提供以下方法： 12345678// 获取共享锁void acquireSharedLock(TransactionId tid, PageId pid);// 获取排他锁void acquireExclusiveLock(TransactionId tid, PageId pid);// 释放锁void releaseLock(TransactionId tid, PageId pid);// 是否持有锁boolean holdsLock(TransactionId tid, PageId pid) 更新后的 BufferPool.getPage 方法如下（因为是页面级锁定，所以只在这里获取锁）： 123456789101112131415&#123; if(perm == Permissions.READ_ONLY)&#123; // 获取共享锁 if(!holdsLock(tid, pid)) &#123; lockManager.acquireSharedLock(tid, pid); &#125; &#125;else&#123; // 获取排他锁 - 存在锁升级情况，所以不判断holdsLock lockManager.acquireExclusiveLock(tid, pid); &#125; ... //缓存中获取页面 ... return page;&#125; 所谓事务获取到锁就是“放行”，获取不到就是“阻塞”。 这里的“锁”其实说成“锁的使用权”或“钥匙”更贴切一点。每个页面其实只有一把锁，需要钥匙才能进入访问。而共享锁，就是说这把锁可以有多把钥匙开启，每一把钥匙给一个事务；排他锁就是只能有一把钥匙给唯一的事务。如果事务获取不到钥匙就被阻塞。其实 Java 中的重量级锁也是这个道理，有时候会被“锁”这个名词给绕晕。 LockManger 的作用就是记录谁拥有某个页面的钥匙，是把什么样的钥匙，为了统一起见，下文仍称“锁”。 接下来是 LockManger 的实现，既然每个页面只有一把锁，并且需要维护这把锁的状态和与事务的关系，那么就可以设计一个 PageLock 类来管理： 12345678910111213141516class PageLock&#123; private PageId pageId; // 页面ID private int lockState; // 0:空闲,-1:排他锁,&gt;0:获取到共享锁的事务数量 CopyOnWriteArrayList&lt;TransactionId&gt; holds; // 获取锁到的事务 public PageLock(PageId pageId)&#123; this.pageId = pageId; holds = new CopyOnWriteArrayList&lt;&gt;(); &#125; // 避免并发修改 public synchronized void stateIncrement(int n)&#123; lockState += n; &#125; public synchronized int getLockState()&#123; return lockState; &#125;&#125; 用一个 lockState 记录这个页面锁的状态。等于 0 代表该页面是空闲的，没有事务访问（无人持锁）；等于 -1 代表该页面的锁为排他锁；大于 0 代表该页面的锁为共享锁，具体数字表示有多少事务正在共享该锁。holds 记录了都是哪些（个）事务获取到该锁。 在 LockManger 中，我们用一个 Map 记录页面和锁的对应关系；为了方便查询，同样用一个 Map 记录事务和其所持有的锁集合的对应关系： 12private Map&lt;PageId, PageLock&gt; pageLocks;private Map&lt;TransactionId, List&lt;PageId&gt;&gt; lookups; 在实现“阻塞”效果时，采用了 wait/notify 的方式，也可采用 JUC 中的各种合适的工具类。注意如果仅仅是为了实现读写锁的话，不需要我们自己记录哪些事务陷入了等待，因为这些工具内部已经实现了记录阻塞线程的逻辑，可以在需要时唤醒。但是在后面实现死锁检测的时候，还是需要记录的。 LockManager 需要对外提供的四个方法实现如下：申请共享锁： 12345678910111213141516171819202122232425public void acquireSharedLock(TransactionId tid, PageId pid) throws TransactionAbortedException &#123; // 拿到页面对应的锁，如果还没有就新建一个 PageLock pageLock = getPageLock(pid); synchronized(pageLock)&#123; // 是排他锁且不是同一个事务（如果是同一个事务直接放行） while(pageLock.getLockState() == -1 &amp;&amp; !pageLock.holds.get(0).equals(tid))&#123; try &#123; pageLock.wait(); // 阻塞 &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; if(pageLock.getLockState() &gt; 0 &amp;&amp; pageLock.holds.contains(tid))&#123; // 重入共享锁 - 不记录 return; &#125; // 锁空闲、是共享锁、有排他申请共享，这几种都放行 // 获取到锁后，记录已获取状态 pageLock.stateIncrement(1); // 共享数量+1 pageLock.holds.add(tid); addToLookups(tid, pid); &#125;&#125; 申请排他锁： 1234567891011121314151617181920212223242526272829public void acquireExclusiveLock(TransactionId tid, PageId pid) throws TransactionAbortedException &#123; PageLock pageLock = getPageLock(pid); synchronized(pageLock)&#123; // 只要锁不空闲，就不能获取排他锁（除非锁升级） while(pageLock.getLockState() != 0)&#123; // 该事务已经获取了共享锁，且它独占 if(pageLock.getLockState() == 1 &amp;&amp; pageLock.holds.get(0).equals(tid))&#123; // 升级为排他锁 - 放行且不记录 pageLock.stateIncrement(-2); // 此时lockState变-1 return; &#125;else if(pageLock.getLockState() == -1 &amp;&amp; pageLock.holds.get(0).equals(tid))&#123; // 该事务已经获取了排他锁，又重入 - 放行且不记录 return; &#125; try &#123; // 否则阻塞 pageLock.wait(); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; // 获取到锁后，记录已获取状态 pageLock.stateIncrement(-1); pageLock.holds.add(tid); addToLookups(tid, pid); &#125;&#125; 这里直接在方法上加 synchronized 也是可行的，但是这样需要每次 notifyAll 所有阻塞的线程，针对性不强。因为每个页面有一个锁，不妨对 pageLock 加锁，这样每次只需 notify 一个阻塞在本页面的线程即可。注意后者需要在 wait 的时候设定等待超时时间，因为会出现别的线程先 notify 后，本线程才进入 wait 的情况，会永久阻塞下去，而设置超时时间后就会不停的循环判断锁条件。这是 wait/notify 方法的固有问题，如果想避免可以用 Semaphore 等其他工具。 （搞错了，没有这个问题，因为 PageLock 已经互斥访问了） 释放锁： 12345678910111213141516171819public void releaseLock(TransactionId tid, PageId pid)&#123; PageLock pageLock = getPageLock(pid); synchronized(pageLock)&#123; pageLock.holds.remove(tid); // 从持有者中去除 removeFromLookups(tid, pid); // 从查询表中去除 if(pageLock.getLockState() == -1) &#123; // 如果当前为排他锁，更新为空闲 pageLock.stateIncrement(1); &#125;else if(pageLock.getLockState() &gt; 0)&#123; // 如果当前为共享锁，持有数-1 pageLock.stateIncrement(-1); &#125; // 当没有事务拿着锁了（空闲状态），或只有一个事务拿着锁（可能有锁升级不成功从而等待的情况） if(pageLock.getLockState()==0||pageLock.getLockState()==1)&#123; // 唤醒该页面阻塞的某个事务去竞争空闲锁或升级锁 pageLock.notify(); &#125; &#125;&#125; 死锁检测检测死锁通常是通过事务之间的等待关系图是否有回路（循环等待）来判断，具体的方法有两种： 拓扑排序：反复寻找一个入度为 0 的顶点，将顶点从图中删除并同时删除它的所有出边，如果最终图中全部剩下入度为 1 的顶点，则图中有回路；如果最终全部顶点都被删除，则不包含回路。 DFS：从所有的点开始进行深度优先搜索，如果一条 DFS 路线中有顶点被第二次访问到，则图中有回路，否则不包含回路。 本实验采用 DFS 方法。 设计死锁检测器类 DeadlockDetector： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class DeadlockDetector &#123; // 图的邻接表 private Map&lt;TransactionId, List&lt;TransactionId&gt;&gt; adjList; // 顶点状态 - null/0:未访问，1:已访问，2:在递归栈内 private Map&lt;TransactionId, Integer&gt; nodeState; public DeadlockDetector()&#123; adjList = new ConcurrentHashMap&lt;&gt;(); nodeState = new ConcurrentHashMap&lt;&gt;(); &#125; // 阻塞将要发生 - 进行记录 public void blockOccurs(TransactionId tid, List&lt;TransactionId&gt; listToWait)&#123; adjList.put(tid, listToWait); &#125; // 事务被唤醒 - 删除记录 public void notified(TransactionId tid)&#123; adjList.remove(tid); &#125; // DFS检测是否有回路 public boolean detectCycle()&#123; nodeState.clear(); for(TransactionId tid:adjList.keySet())&#123; if(dfs(tid))&#123; return true; &#125; &#125; return false; &#125; private boolean dfs(TransactionId tid)&#123; nodeState.put(tid, 2); // 标记入递归栈 List&lt;TransactionId&gt; adj = adjList.get(tid); if(adj != null)&#123; for(TransactionId t:adj)&#123; // 跳过自反边的情况 - 单个锁升级等待不算死锁 if(tid.equals(t)) continue; int state = nodeState.getOrDefault(t, 0); if(state == 2)&#123; return true; // 找到环 &#125;else if(state == 0 &amp;&amp; dfs(t))&#123; // 递归 return true; // 找到环 &#125; &#125; &#125; nodeState.put(tid, 1); // 出递归栈，标记已访问 return false; &#125; 为了简便，不再设计顶点类，而是让每一个 TransactionId 代表自己的顶点，采用图的邻接表表示法。nodeState 记录 DFS 中顶点的状态。 每当发生一个阻塞就调用 blockOccurs 方法，因为是页面级的锁定，所以一个事务陷入阻塞后一定等待的是持有页面锁的所有事务，也就是 PageLock 里面 holds 列表所存储的事务。所以我们只需每次将 holds 传入第二个参数，当做该事务顶点的所有出边（表示等待）即可。当事务获得锁（或者发现死锁）后，调用 notified 方法删除该顶点的所有出边。 更新获取共享锁的代码如下（循环部分）： 1234567891011121314151617...while(pageLock.getLockState() == -1 &amp;&amp; !pageLock.holds.get(0).equals(tid))&#123; deadlockDetector.blockOccurs(tid, pageLock.holds); // 添加等待边 if(deadlockDetector.detectCycle())&#123; // 检测到死锁 deadlockDetector.notified(tid); // 移除等待边 // 抛出异常，SimpleDB会abort该事务 throw new TransactionAbortedException(); &#125; try &#123; pageLock.wait(10); deadlockDetector.notified(tid); // 移除等待边 &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125;&#125;... 获取排他锁同理。 DEBUG 记录 PageCache 中的 page 数量可能会小于 LockManger 中的 page 数量，所以在根据 LockManager 中的 Page 来 flush 的时候需要进行 null 判断。 在 Transaction system test 的 10 个线程测试中，出现了时而成功时而永久阻塞的情况。经过调试， 发现是在 flushAllPages() 的循环里卡住； 发现 pageCache 返回的 Iterator 会不停的给出同一个 next 页面，死循环； 发现原因是自定义的双向链表尾结点 tail 丢失链接，导致无法停止遍历； 发现是没有注意线程安全的问题。在之前实现 LRUBasedPageCache 的时候没有使用 ConcurrentHashMap 类和 synchronized 关键字，导致并发问题，修改之后就没有问题了。 多线程情况下，所有的 Map 都最好用 ConcurrentHashMap，List 最好用 CopyOnWriteArrayList，它们除了是线程安全的，还支持遍历时修改，不会报并发修改异常。 在实现死锁检测的时候要注意：当一个事务已经获取共享锁，又要升级为排他锁，此时如果共享数不为 1，那么就要阻塞，但这个时候就会出现自己等待自己的情况（等待图中体现为自反边），但这不是死锁（因为其他的共享锁事务在 release 时会进行唤醒，当共享数为 1 时就不继续等待了），不应该被识别。所以在 DFS 的时候要跳过自反边。 Lab 仓库地址：zyrate/simple-db-hw-2021 (github.com)","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"mit6830","slug":"mit6830","permalink":"https://zyrate.github.io/tags/mit6830/"}]},{"title":"MIT6.830 SimpleDB 实现笔记 Lab 3","slug":"830 Lab 3","date":"2023-09-26T16:00:00.000Z","updated":"2024-03-12T09:59:47.980Z","comments":true,"path":"2023/09/27/830 Lab 3/","link":"","permalink":"https://zyrate.github.io/2023/09/27/830%20Lab%203/","excerpt":"","text":"上一个 Lab 完成的是查询过程，Lab 3 的内容是查询优化（Query Optimization），主要完成两个部分，表统计信息 TableStats 和连接优化器 JoinOptimizer。 Cost 模型在一次查询中，最耗时的部分就是多表 Join，然而采用不同的 Join 顺序的效率差别很大，这就需要进行优化了。本 Lab 基于成本（Cost）模型进行查询优化，分别关注scancost 和 joincost。 在 Lab 3 给出的说明文字中，关于 joincost 在 Cost 总述和嵌套 Join 的 Cost 处的含义有一些出入（一个不包含 scan，一个包含 scan），为了避免引起歧义，统一采用如下的描述： scancost 是扫描一个数据表所需要的时间，大部分由磁盘 I/O 消耗。 joincost 是连接两个数据表所需要的总时间，不同的连接方式计算公式不一样，它包含 scancost,由 I/O 和 CPU 消耗。 scancost 的计算公式如下： 1scancost(t1) = num_pages(t1) * io_cost_per_page 当我们采用嵌套循环连接时， joincost 的计算公式如下： 12joincost(t1 join t2) = scancost(t1) + ntups(t1) * scancost(t2) //IO cost + ntups(t1) * ntups(t2) //CPU cost scancost 取决于硬件速度和表的大小，joincost 取决于 scancost 和表的连接基数（cardinality）（在上述公式是 ntups） ，基数取决于谓词的选择性（selectivity），因此想要计算 joincost 就必须对每个表的选择性进行估计。 过滤选择性（Filter Selectivity）谓词过滤选择性（范围0-1）指的是表中的元组通过过滤谓词的比例。选择性越大，表的基数越大，反之则越小。 在 SimpleDB 中，每个表会有一个 TableStats 对象，维护该表的统计信息，其中包含 scancost 估计和某个谓词对该表某一列的 selectivity 估计。连接优化器（JoinOptimizer）会调用 TableStats 的方法获取目标表的相关数据，根据公式计算连接 Cost，以确定一个最优的（代价最小的）连接顺序（查询计划）。 关键问题在于如何对 selectivity 进行估计。最常用的方法是使用直方图（Histogram）。 具体来说，就是对该表的每一列建立一个直方图，每个直方图将该列从最小值到最大值分成若干个区间，每一个区间记录了落在该区间的元组数量。这样，在面对一个谓词时，我们可以很快地计算出通过该谓词过滤的大致元组数量。示意图如下： image.png 对于一个常量 const，假设它落到直方图某个区间的桶（bucket）高度为 h_b，宽度为 w_b，表中元组总数量为 ntups，那么： 对于谓词 f=const，选择性估计公式为：(h_b/w_b) / ntups。 对于谓词 f&gt;const，如上图的情况所示，阴影部分就是通过谓词过滤的部分，它分为两部分：在桶 b 内的阴影部分、桶 b 右侧的所有桶。b 右侧的所有桶的选择性的计算公式都和 f=const 一样，最后进行累加即可；而对于桶 b 内阴影部分的选择性，计算公式为：(h_b/ntups) / ((b_right-const)/w_b)，这个公式假设在桶 b 内元组均匀分布。 对于谓词 f&lt;const，与上同理，对于桶 b 内阴影部分的选择性，计算公式为：(h_b/ntups) / ((const-b_left)/w_b)。 对于&gt;=谓词，只需要把&gt;和=的选择性相加即可，&lt;=同理。 IntHistogram 实现要实现直方图，比较直观的方式是将每一个桶看做一个对象，这样在获取桶高和桶的左右边界时很方便。整个直方图就是一个桶数组。 首先新建一个内部类：Bucket 1234567891011121314public class Bucket &#123; public double left; // 这里要用double，因为max-min可能很小 public double right; public int height; public Bucket()&#123; this.height = 0; &#125; // 填充该桶，高度+1 public void populate()&#123; this.height++; &#125; &#125;// Bucket 数组private final Bucket[] histData; 实现选择性估计方法： 123456789101112131415161718192021222324252627public double estimateSelectivity(Predicate.Op op, int v) &#123; double selectivity = -1.0; // 注意查询的值不一定在最大最小区间内 // 结构化、简化代码，减少出错 switch (op) &#123; case EQUALS: selectivity = calculateEquals(v); break; case GREATER_THAN: selectivity = 1 - calculateLessThan(v) - calculateEquals(v); break; case GREATER_THAN_OR_EQ: selectivity = 1 - calculateLessThan(v); break; case LESS_THAN: selectivity = calculateLessThan(v); break; case LESS_THAN_OR_EQ: selectivity = calculateLessThan(v) + calculateEquals(v); break; case NOT_EQUALS: // 1 - Selectivity(=) selectivity = 1 - calculateEquals(v); break; &#125; return selectivity; &#125; 这里，把关键的计算过程封装成了其他函数，其实可以发现只需要实现等于和小于的选择性估计即可，其他的谓词都可以通过这两种推算出来。 估算等于的选择性： 123456private double calculateEquals(int v)&#123; int i = findBucketIndex(v); if (i &lt; 0) return 0; // 这里必须要+1，杜绝零点几的桶宽出现 return (double) histData[i].height / (((int)bWidth+1) * ntups); &#125; 估算小于的选择性： 1234567891011121314151617private double calculateLessThan(int v)&#123; int index = findBucketIndex(v); double selectivity = 0; if(index == -1)&#123; // 在最左侧 return 0; &#125;else if(index == -2)&#123; // 在最右侧 return 1; &#125;else&#123; // 在区间内，先算桶内的部分 selectivity = (double) histData[index].height / ntups * (v - histData[index].left) / bWidth; &#125; // 在算桶外的部分 for(int i=0; i&lt;index; i++)&#123; selectivity += (double) histData[i].height / ntups; &#125; return selectivity; &#125; StringHistogram 采用了一种简便的实现方式，即“套壳”IntHistogram。所有的 String 数据被按照一定规则转化为 int 类型，然后使用整型直方图去管理和统计。 TableStats 实现有了直方图这个工具后，对表的信息统计就很方便了，每一个 Table 会附带一个 TableStats，在初始化的时候就对该表的每一列新建一个直方图存储起来，当然这个直方图并不能实时更新，因为它的范围已经固定了，但是在一段时间内，它能够反应该表数据的大致分布情况，从而为查询做出优化。TableStats 需要根据不同的策略进行重建。 在 TableStats 初始化时，需要扫描两遍 Table： 第一遍计算每一列的最大最小值 第二遍为每一列新建一个直方图 Join Ordering有了 TableStats 的各项统计信息，我们就可以对一个 Join 进行优化了，所谓的优化其实就是生成一个代价最小的 Join 执行计划。 对于一系列需要 Join 的逻辑表对（LogicalJoinNode），我们采用 Selinger 算法进行动态规划查找，找到代价最小的计划。 1234567891011121314151617181920212223242526272829303132333435public List&lt;LogicalJoinNode&gt; orderJoins( Map&lt;String, TableStats&gt; stats, Map&lt;String, Double&gt; filterSelectivities, boolean explain) throws ParsingException &#123; // 连接对 Set&lt;LogicalJoinNode&gt; joinSet = new HashSet&lt;&gt;(joins); // join order 缓存 PlanCache planCache = new PlanCache(); int size = joinSet.size(); for(int i=1; i&lt;=size; i++)&#123; // 遍历子集合 Set&lt;Set&lt;LogicalJoinNode&gt;&gt; sets = enumerateSubsets(joins, i); for(Set&lt;LogicalJoinNode&gt; set:sets)&#123; double bestCost = Double.MAX_VALUE; CostCard bestPlan = null; // 找到本次bestPlan for(LogicalJoinNode joinNode:set)&#123; CostCard costCard = computeCostAndCardOfSubplan(stats, filterSelectivities, joinNode, set, bestCost, planCache); if(costCard != null &amp;&amp; costCard.cost &lt; bestCost)&#123; bestCost = costCard.cost; bestPlan = costCard; // 这里面包含着计划 &#125; &#125; if(bestPlan != null)&#123; planCache.addPlan(set, bestPlan.cost, bestPlan.card, bestPlan.plan); &#125; &#125; &#125; if(explain)&#123; printJoins(planCache.getOrder(joinSet), planCache, stats, filterSelectivities); &#125; // 返回最优的join order return planCache.getOrder(joinSet); &#125; image.png Lab 仓库地址：zyrate/simple-db-hw-2021 (github.com)","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"mit6830","slug":"mit6830","permalink":"https://zyrate.github.io/tags/mit6830/"}]},{"title":"MIT6.830 SimpleDB 实现笔记 Lab 2","slug":"830 Lab 2","date":"2023-09-07T16:00:00.000Z","updated":"2024-03-12T09:59:51.232Z","comments":true,"path":"2023/09/08/830 Lab 2/","link":"","permalink":"https://zyrate.github.io/2023/09/08/830%20Lab%202/","excerpt":"","text":"Lab 2 和 Lab 3 都与数据库的查询过程有关，在执行查询的过程中会先后生成LogicalPlan和PhysicalPlan。逻辑计划由一系列的逻辑算子结点列表组成，它保存了需要进行 Scan、Join、Filter 等操作的表名、列名、谓词等信息。由逻辑计划生成的物理计划其实就是一系列物理算子嵌套形成的结点树。 火山模型SimpleDB 采用的是最经典且最广泛使用的查询模型：火山模型（Volcano），也叫流水线模型（Pipeline）。 该模型要求：每一个物理计划算子（Operator）都要实现 next() 方法，在该方法中，循环调用它的 child 算子的 next 方法以获取元组并进行数据处理，根据本算子的逻辑返回给父算子一个元组。直到 child 没有元组可获取，则返回 null。因此数据是从最底层数据表，一层一层的经过中间算子的处理、过滤，被传递到顶层的客户端的，因此被形象地叫做“火山模型”。而“流水线”的意思是，每当父算子调用 child 的 next 方法后，如果它想调用下一次 next，就只能等待这一次获取的数据经过物理计划自底向上的“流水线算子”的处理直至“涌出”，这期间父算子无法做其他事情。 火山模型的优点是每一层的算子只需要无脑从子算子获取元组，并根据自己的逻辑考虑如何返回元组给父算子，而不需要关心父算子和子算子具体的逻辑和实现。 在 SimpleDB 中，最顶层的算子是 Project（投影），它负责把所有的结果元组按照查询要求只显示指定的 Field 列；最底层的算子是 SeqScan（顺序扫），它负责从指定的数据表中一行一行的顺序读取元组；在这两者之间的算子有：Aggregate, Filter, Join, OrderBy, Insert, Delete 等。其中 Insert 和 Delete 比较特殊，因为他们不从数据表读取数据，而是从要插入或删除的元组集合中读取数据。 Lab2 总共有 5 个 exercise，主要练习了与执行计划相关的各种 execution 操作。比如过滤、连接、聚合、插入、删除等操作。每一个操作算子都继承了 Operator（OpIterator）类，它们会： 接受一个 child OpIterator，用以读取（遍历）目标数据； 接受一些控制该算子的参数； 同时实现 hasNext()、next() 等供外界遍历的方法。 Exercise 1 src/java/simpledb/execution/Predicate.java src/java/simpledb/execution/JoinPredicate.java src/java/simpledb/execution/Filter.java src/java/simpledb/execution/Join.java 谓词和连接谓词的作用是根据指定的 field 、操作符和操作数来判断某一个 Tuple 是否需要过滤。而 Filter 和 Join 算子则遍历 child 数据，利用上述谓词来进行来进行过滤，返回留下来的 Tuple。实现起来比较简单。 这里有个注意的点是，每个实现了 Operator 的算子都要重写 getTupleDesc() 方法，生成该算子每次 next() 后返回的元组结构描述。比如 Join 算子返回的是两个元组合并后的结构： 123public TupleDesc getTupleDesc() &#123; return TupleDesc.merge(child1.getTupleDesc(), child2.getTupleDesc());&#125; Exercise 2 src/java/simpledb/execution/IntegerAggregator.java src/java/simpledb/execution/StringAggregator.java src/java/simpledb/execution/Aggregate.java IntegerAggregater 和 StringAggregator 是具体类型的聚合器，它们的作用是在聚合算子 Aggregate 遍历表的过程中统计分组（group）信息并得到最终聚合结果。Integer 类型有五种基本聚合操作：MIN, MAX, SUM, AVG, COUNT，而 String 类型只有 COUNT 一种。不同的聚合操作进行不同的计算即可。 Exercise 3, 4 src/java/simpledb/storage/HeapPage.java src/java/simpledb/storage/HeapFile.java src/simpledb/BufferPool.java insertTuple() deleteTuple() src/java/simpledb/execution/Insert.java src/java/simpledb/execution/Delete.java 练习 3 和练习 4 要求实现 HeapFile 和 HeapPage 的可变性，即可以随时插入、删除元组，并且实现 Insert 和 Delete 算子。 首先，向 HeapPage 中插入元组需要根据 header 标志位找到一个空闲 slot，在插入后（数组赋值）header 对应位置标志为 1；删除元组则反之。因为 header 使用 byte 数组存储的，所以需要一定算法将对应 byte 取出更改某一位值后再放回： 123456789101112131415private void markSlotUsed(int i, boolean value) &#123; byte markBit = value?(byte)1:0; byte oldByte = header[i/8]; byte newByte = (byte) 0 ; for(int pos=7; pos&gt;=0; pos--)&#123; // 这里注意顺序 byte originBit = (byte) (oldByte &gt;&gt; pos &amp; 1); // 不变的bit if(pos == i%8)&#123; // 到了要设置的bit newByte |= markBit; &#125;else&#123; newByte |= originBit; &#125; newByte &lt;&lt;= pos!=0?1:0; // 除了最后一位，填充后左移 &#125; header[i/8] = newByte;&#125; HeapFile 的插入方法需要遍历所有的 HeapPage，判断页面是否有空闲 slot，如果有的话，调用该页的 insert 方法，如果所有页面都无空闲，就要新建一个页面再行插入。 而最终所有的应用程序在插入元组时，是调用 BufferPool 的方法。BufferPool 调用 HeapFile 的 insert 方法，接收一个 Page 列表，存储所有被影响的页面（如果不考虑副本的话只有 1 个页面）。这些页面就是所谓的脏页（dirty page），即在缓存中发生了改动但还没有同步到硬盘中的页面。BufferPool 需要将这些页面标记为“脏页”。 BufferPool、HeapFile、HeapPage 之间必须遵循固定的调用关系： image.png Exercise 5 src/java/simpledb/storage/BufferPool.java evictPage() 练习 5 要求在缓冲区满了以后实现页面置换算法。因为之前没有考虑这个功能所以 getPage() 方法要重新写。 最常见的置换算法是 LRU（最近最久未使用算法），实现它可以用一个 List，每次访问一个页面就把它放到表头，这样需要同置换时，表尾的页面就是最近最久未使用的，直接逐出。然而仅仅用一个 List，在访问页面时还需要遍历查找，不如 HashMap 高效，但是仅仅用 HashMap 又无法实现算法要求。 所以我将两者结合，自定义了一个 PageCache 接口和 LRUBasedCache 实现类，手动实现双向链表，结合 HashMap，实现了 O(1) 复杂度 GET、PUT 操作和灵活置换的 LRU 算法。 页面缓存接口 PageCache： 123456789101112131415161718public interface PageCache &#123; // 向缓存中添加页面 void putPage(Page page); // 系统内部获取页面 - LRU不生效 Page getPage(PageId pid); // 外部（事务）获取页面 - LRU生效 Page accessPage(PageId pid); // 从缓存中删除页面 void removePage(PageId pid); // 缓存是否已满 boolean isFull(); // 下一个要被置换的页面PID PageId pidToBeEvicted(); // 置换页面（删除） void evictPage(); // 页面迭代器 Iterator&lt;Page&gt; iterator(); &#125; 实现类 LRUBasedCache 主要就是维护一个双向链表和一个 PageId 到链表节点的映射，然后在 accessPage 的时候实现 LRU 规则（将被访问的节点向链表头移动）： 12345678910111213141516171819202122232425262728293031323334353637383940public class LRUBasedCache implements PageCache&#123; /** * 双向链表结点 */ private static class Node&#123; Page page; Node pre; Node next; public Node(Page page)&#123; this.page = page; &#125; &#125; private final int capacity; private Map&lt;PageId, Node&gt; map; private Node head; private Node tail; ... @Override public synchronized Page accessPage(PageId pid) &#123; Node node = map.get(pid); if(node == null)&#123; return null; &#125; moveToHead(node); // LRU算法 - 向链表头部移动 return node.page; &#125; @Override public synchronized PageId pidToBeEvicted() &#123; Node n = tail.pre; while(n != head)&#123; if(n.page.isDirty() != null)&#123; // 确保不是脏页（no-steal规则） n = n.pre; continue; &#125; break; &#125; return n==head?null:n.page.getId(); // 返回null代表全都是脏页 &#125; ...&#125; image.png image.png Lab 仓库地址：zyrate/simple-db-hw-2021 (github.com)","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"mit6830","slug":"mit6830","permalink":"https://zyrate.github.io/tags/mit6830/"}]},{"title":"MIT6.830 SimpleDB 实现笔记 Lab 1","slug":"830 Lab 1","date":"2023-08-08T16:00:00.000Z","updated":"2023-12-09T12:37:06.570Z","comments":true,"path":"2023/08/09/830 Lab 1/","link":"","permalink":"https://zyrate.github.io/2023/08/09/830%20Lab%201/","excerpt":"","text":"6.830是麻省理工学院（简称 MIT）的一门计算机科学课程，全名为”6.830: Database Systems”。该课程是关于数据库系统的高级课程，旨在教授学生关于数据库的设计、实现和优化的知识和技能。 课程附带了6个Lab以供练习，最终目的是使学生能够用Java写出一个简易数据库系统，这6个Lab由浅入深，覆盖了数据库的核心知识点。 Lab1 总共有 6 个 exercise，主要是练习的是数据库的数据存储部分。 Exercise 1 src/java/simpledb/storage/TupleDesc.java src/java/simpledb/storage/Tuple.java 在 SimpleDB 中，逻辑上的存储单元由大到小分别是 Database -&gt; Table -&gt; Tuple -&gt; Field。一个表中的每一条记录就是一个 Tuple 元组对象，元组中的每一列是一个 Field 字段值，目前只实现了 Int 和 String（固定长度）类型。 每个元组需要一个 TupleDesc 对象来描述该元组包含的所有字段，包括每个字段类型 fieldType 和字段名 fieldName，在 SimpleDB 中由 TDItem 对象存储。 在实现 TupleDesc 的 toString() 方法时，发现了一个显而易见但之前没注意到的问题：集合每次在调用 iterator() 方法时都会生成一个新的 Iterator，所以不能反复调用此方法。另外 for-each 语句不能用于 Iterator，只能用于数组或实现了 Iterable 接口的对象。 Exercise 2 src/java/simpledb/common/Catalog.java Catalog 是管理数据库所有表的单例对象，比较简单。 主要实现了供外界调用的 addTable、getTableName 等方法 在 SimpleDB 中，一个 Table 对应一个 DbFile，并且共享同一个 ID（DbFile 绝对路径的 hashcode） 这里的 HashMap 用并发安全的比较好 Exercise 3 src/java/simpledb/storage/BufferPool.java 实现 getPage() 方法 BufferPool 也是一个全局单例对象，它负责维护访问页面 Page 的缓存。关于页面，有三个容易混淆的概念： 硬盘中的页面（也可以叫块 block） 操作系统中的页面 数据库中的页面 Page 是数据库向硬盘中读取和写入一次数据的最小单位，一般来说数据库的页面比底层的页面要大一些，所以需要我们自己写一些逻辑来保证操作的原子性（暂时不需要）。 每次通过 PageId（存储 tableId 和 pageNo）来获取页面。首先查找缓存，没有的话就通过 Catalog 获取 DbFile 读取页面并加入缓存。如果缓存占满，就要进行页面置换（暂时不需要）。 1234567891011public Page getPage(TransactionId tid, PageId pid, Permissions perm) throws TransactionAbortedException, DbException &#123; Page res = idToPage.get(pid); if(res == null) &#123; Catalog catalog = Database.getCatalog(); DbFile f = catalog.getDatabaseFile(pid.getTableId()); res = f.readPage(pid); idToPage.put(pid, res); &#125; return res;&#125; Exercise 4 src/java/simpledb/storage/HeapPageId.java （继承自 PageId） src/java/simpledb/storage/RecordId.java src/java/simpledb/storage/HeapPage.java （继承自 Page） 前两个 Id 对象主要就是 hashcode() 和 equals() 方法的编写，注意： 在重写一个类的 equals 方法的时候，必须同时重写 hashCode 方法。否则的话，在使用需要判断 hash 值的数据结构（如 HashMap）进行存储时就会出现问题。要求： equals 为 true 时 hashCode 一定为 true；hashCode 为 true 时，equals 不一定为 true。 HeapPage 是实际存储在缓存中的页面（从硬盘读取到内存），它主要包含头部 header 标志位和一个固定长度的 tuple 数组（slots），结构示意图如下： 页面中的 slot（插槽）有几个，header 就有几位，当一个元组插入 slot 后，header 对应位置设置为 1，删除元组则反之。在 SimpleDB 中，一个 table 的 TupleDesc 确定下来后，tuple 的长度就是固定的，因此可以计算出该页面可以有多少个 slot，用于初始化 header 和数组。 计算 slot 个数： 123456private int getNumTuples() &#123; // 每个页面可存储的元组数计算公式（大小单位是字节）： // 页面大小 * 8 / (元组大小 * 8 + 1)，向下取整 int numTuples = BufferPool.getPageSize()*8 / (td.getSize()*8+1); return numTuples;&#125; +1 是因为每个元组要附带一个标志位。 计算 header 大小（字节），多余的 0 位不考虑： 12345private int getHeaderSize() &#123; // Header要存储numSlots个bit，计算所需的bytes int headerSize = (int) Math.ceil(numSlots / 8.0); // 向上取整 return headerSize;&#125; HeapPage 在初始化时接受一个 pageId 和从硬盘读入的序列化后的 byte 数组进行反序列化，相反，getPageData 方法将该页面序列化以存入硬盘。 需要实现 isSlotUsed() 方法，该方法返回某个 slot 是否插入了元组。检查 header 对应位置的标志位是否为 1 即可。 12345public boolean isSlotUsed(int i) &#123; // 注意规定了从每个byte从右到左（low to high） byte slot = (byte) ((header[i/8] &gt;&gt; (i%8)) &amp; 1); return slot == (byte)1 ? true : false;&#125; Exercise 5 src/java/simpledb/storage/HeapFile.java （继承自 DbFile） HeapFile 对应一个表在硬盘中存储的文件，存储的单位是 HeapPage，所以主要是实现 readPage() 方法，它接受 pageId，需要找到对应 Page 在文件中的偏移量读取出来。一定要用文件随机读取，而不能一次性全部读到内存中，因为文件可能会很大。 12345678910111213141516public Page readPage(PageId pid) &#123; // 找到对应Page所在的偏移量，读取后生成HeapPage int pageSize = BufferPool.getPageSize(); int offset = pid.getPageNumber() * pageSize; byte[] data = new byte[pageSize]; Page heapPage = null; try (RandomAccessFile f = new RandomAccessFile(file, &quot;r&quot;)) &#123; f.seek(offset); f.read(data); heapPage = new HeapPage((HeapPageId)pid, data); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return heapPage;&#125; 另外的一个难点是实现 iterator() 方法，它的功能是遍历该表（DbFile）中的所有元组。那么就需要我们遍历 HeapFile 的所有 HeapPage，过程中使用 HeapPage 的 Iterator 来遍历元组。但是上面实现的 readPage() 方法是给 BufferPool 调用的，因为所有的页面读取都要经过缓存。所以我们需要调用 BufferPool 的 getPage() 方法来获取页面，从 pageNo = 0 开始累加，直到到达该文件所存储的页面数量的上限，是在 numPages() 里计算得到的（文件大小除以 PageSize）。 Exercise 6 src/java/simpledb/execution/SeqScan.java （实现 OpIterator） Operator 是执行计划的基本单位，最简单、最底层的一个 Operator 就是 SeqScan，按照存储顺序扫描某一个表的全部元组。 这里主要添加了表的别名 alias的概念，我们需要生成一个 tableAlias.filedName 形式的 TupleDesc 以供后续使用。 实现 OpIterator 接口的全部方法，主要是调用 HeapFile 里的 Iterator 的相应方法。 详细的数据库查询模型在下一个 Lab 总结。 image.png Lab 仓库地址：zyrate/simple-db-hw-2021 (github.com)","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"mit6830","slug":"mit6830","permalink":"https://zyrate.github.io/tags/mit6830/"}]},{"title":"OSPP 2023 开源之夏 DolphinScheduler 项目申请书","slug":"OSPP 2023","date":"2023-06-09T16:00:00.000Z","updated":"2024-03-02T08:09:59.000Z","comments":true,"path":"2023/06/10/OSPP 2023/","link":"","permalink":"https://zyrate.github.io/2023/06/10/OSPP%202023/","excerpt":"","text":"项目名称： DolphinScheduler Listener 机制项目编号： 232290593项目社区导师： 孙朝和老师申请人： 郑云瑞GitHub ID： @zyrate邮箱： &#112;&#x6f;&#x6c;&#x6c;&#101;&#114;&#x79;&#x38;&#x38;&#x38;&#x40;&#103;&#x6d;&#x61;&#105;&#108;&#46;&#x63;&#x6f;&#x6d;教育经历： 中国地质大学（武汉）电子信息专业硕士在读 1 项目描述1.1 项目简述当工作流/任务执行时，第三方系统希望可以通过接收消息的方式（而非轮询）来实时获取工作流/任务的状态和相关信息，可以通过添加自定义 Listener 的方式来实现获取需要的信息推送到特定的目的地（消息队列、外部系统接口等）的目标。 1.2 项目产出要求 完成 Listener 机制设计和开发、并将代码提交到开发分支上 详细的设计文档和使用文档 详尽的 UT 测试类2 项目总体设计2.1 Listener 机制 Listener 机制是一种用于实现异步消息传递和事件处理的机制，广泛用于各种系统之中方便开发人员内部开发以及用户外部对接使用。它总体采用“订阅”的思想，通过 Listener 去监听事件的产生。当事件发生时，Listener 的对应方法将被触发，从而执行用户实现定义好的操作。 在众多系统中，Apache Spark 的 Listener 机制在分布式系统中较为成功，它允许用户自定义 Listener 来监听运行流程中各个节点的状态；提出了 ListenerBus 的概念，让所有的监听器在独立于主线程的线程中处理事件，提高了资源利用率与容错性；并且可以根据事件的类型创建不同的异步事件队列。以上种种优点都十分契合 DolphinScheduler 工作流/任务状态事件通知的场景，所以我们类比 Spark Listener 的机制来进行设计。 初步设计 DolphinScheduler Listener 机制示意图如下： Listener.jpg|700 核心类有三个： 一个 EventListener 对象拥有所有事件触发后将被调用的方法（onXXX），用户只需继承这个类在重写方法里进行消息通知； 一个 ListenerBus 对象维护一个 listener Map（采用 Map 是为了通过 ID 移除 listener 较为方便）和一个 event 队列，在开启监听后不断地将队列里相应的事件转发给对应的 listener； 一个唯一的 ListenerBusManager 对象负责最外层的调用（触发事件、listener 注册与卸载）。 借鉴 Spark Listener 的设计思想，我们可以在 ListenerBusManager 中管理多个 ListenerBus，比如负责对外向用户提供注册功能的 Bus 和对内向开发人员提供注册功能的 Bus，这样如果某一个异步队列崩了的话，不会影响另外的队列，能够增强机制的健壮性。 2.2 热加载功能 实现 Listener 插件热加载功能可以在系统运行的时候动态地注册/卸载自定义 Listener，方便用户随时开始和暂停监听工作流与任务的状态。 和导师讨论后，拟采用类似 DolphinScheduler 现有的 UDF 管理模块的形式设计 Listener 管理模块，以实现用户自定义 Listener 的热加载： image.png|700 Listener 管理同样拥有两个子模块：资源管理和监听管理。 用户首先将自定义 Listener 的jar 包在 资源管理 上传，并填写该 jar 包的相关信息，如：文件名称、描述等； 用户在 监听管理 点击“创建 Listener”，并填写该 Listener 的相关信息，如：Listener 名称、包名类名、Listener jar 包资源、监听说明等； 创建完成后，用户选择某个 Listener 点击 注册，系统将开始对此 Listener 开展分布式加载（因为要保证该 Listener 被添加到每个 master 节点的 ListenerBus 中），加载完成后即开始监听； 用户可以随时点击某个 Listener 的 卸载 操作，系统对此 Listener 进行分布式卸载（即从所有节点的 ListenerBus 中删除该 Listener），即停止监听。 3 项目详细设计3.1 数据表设计新增 Listener 数据表：t_ds_listeners 列名 类型 描述 主键 外键 id int listener 唯一 ID √ user_id int 创建该 listener 的用户 ID √ listener_name varchar listener 名称 class_name varchar 包名类名 description varchar listener 功能描述 resource_id int jar 包资源 ID √ resource_name varchar jar 包资源名称 state int listener 当前状态 （未注册、注册中、已注册、卸载中） create_time datetime 创建时间 update_time datetime 更新时间 3.2 API 设计POST ： /dolphinscheduler/resources/listener创建 ListenerPUT ： /dolphinscheduler/resources/listener/&#123;id&#125;更新 ListenerGET ： /dolphinscheduler/resources/listener查询 Listener 列表（分页）GET ： /dolphinscheduler/resources/&#123;id&#125;/listener查询某个 Listener 的详细信息DELETE ： /dolphinscheduler/resources/listener/&#123;id&#125;删除某个 ListenerGET ：/dolphinscheduler/resources/listener/verify-name查询某个 Listener 名称是否可用PUT ： /dolphinscheduler/resources/listener/&#123;id&#125;/register将某个 Listener 进行注册PUT ： /dolphinscheduler/resources/listener/&#123;id&#125;/unregister将某个 Listener 卸载 3.3 Event 设计目前设计了如下的事件类（后续可能会添加）： 事件类 触发时刻（加入 ListenerBus） 触发于代码位置 (org/apache/dolphinscheduler/server/master/) EventWorkflowSubmitted master 处理 START_WORKFLOW 事件时，提交工作流实例成功后 event/WorkflowStartEventHandler. java: 65 EventWorkflowBlocked master 在执行 handleStateEvent 时, 执行 processBlock () 后 event/WorkflowBlockStateEventHandler. java: 50 EventWorkflowStopped master 在执行 handleStateEvent 时，isStop ()为 true event/WorkflowStateEventHandler. java: 50 EventWorkflowFinished master 在执行 handleStateEvent 时，isFinished ()为 true event/WorkflowStateEventHandler. java: 63 EventTaskSubmitted master 在提交下一个 node 的时候，创建 task 实例并加入列表 runner/WorkflowExecuteRunnable. java: 1352 EventTaskPaused master 接收到任务暂停的 rpc 命令 processor/MasterTaskPauseProcessor. java: 58 EventTaskFinished master 接收到 worker 传来的 task 执行结果后 processor/TaskExecuteResultProcessor. java: 63 EventTaskKilled master 接收到 kill 命令，执行 cancel 后 processor/MasterTaskKillProcessor. java: 69 所有事件类继承自父类 ListenerEvent，重写 getEventType 方法。 每个事件类拥有用户所关心的该事件的相关信息和上下文信息。 3.4 ListenerBus 设计ListenerBus 是 Listener 机制中最核心的概念，它负责异步事件队列的维护和 EventListener 实例的维护。 在 start ()后开始接受事件，并开始分发 在 stop ()时，进行优雅地停止线程 简要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class ListenerBus&#123; private ConcurrentHashMap&lt;Integer, EventListener&gt; listeners = new ConcurrentHashMap(); private LinkedBlockingQueue&lt;ListenerEvent&gt; queue = new LinkedBlockingQueue(); private boolean isStarted = false; private Thread dispatchThread; // 事件分发线程 @SneakyThrows private void dispatch()&#123; ListenerEvent nextEvent = queue.take(); // 消费事件，直到收到停止命令 while(!nextEvent.getEventType().equals(&quot;STOP_LISTENERBUS&quot;))&#123; for(EventListener listener : listeners.values())&#123; handleEvent(listener, nextEvent); &#125; nextEvent = queue.take(); &#125; &#125; public synchronized void start()&#123; if(!isStarted)&#123; dispatchThread = new Thread()&#123; @Override public void run() &#123; dispatch(); &#125; &#125;; dispatchThread.start(); isStarted = true; &#125; &#125; @SneakyThrows public synchronized void stop()&#123; if(isStarted) &#123; postEvent(new StopListenerBus()); // 将停止命令推入队列 dispatchThread.join(); // 会等待将当前队列中剩余事件分发完成 isStarted = false; &#125; &#125; public void addListener(EventListener listener)&#123; listeners.put(listener.getListenerId(), listener); &#125; @SneakyThrows public void postEvent(ListenerEvent event)&#123; if(isStarted) &#123; queue.put(event); &#125; &#125; // 将事件分发给对应的Listener public void handleEvent(EventListener listener, ListenerEvent event)&#123; switch(event.getEventType())&#123; case &quot;WORKFLOW_SUBMITTED&quot;: // 类型可以是枚举 listener.onWorkflowSubmitted((EventWorkflowSubmitted) event); break; case &quot;TASK_FINISHED&quot;: listener.onTaskFinished((EventTaskFinished) event); break; // 其他 event types... &#125; &#125; &#125; 3.5 分布式加载设计 分布式加载和卸载需要考虑到多个 master 节点的一致性，即要保证将用户自定义 Listener 添加到所有 master 节点的 ListenerBus 中去，使得每个 master 负责的工作流都能够被监听到。如果在注册过程中，某个节点在 jar 包加载和实例获取的过程中出现了错误，无法继续添加，则所有已添加的节点应当将该 Listener 移除，并返回给用户错误信息。 另外，会出现多个用户同时请求注册 Listener 或一个用户同时注册请求多个 Listener 的情况。这就要求每次请求的分布式加载互不干扰，所以设计了临时处理对象 RegisterHandler 和 UnregisterHandler 来保存每个加载过程的上下文信息（如本次注册已成功的 master 个数等）。 下面描述注册和卸载流程，前置操作是：用户已在资源中心将 jar 包上传，并创建 Listener 记录。注册流程： 用户点击注册某 listener，为了并发时不发生冲突，API（service 层）每次请求新建一个 register handler 对象临时保存到 map &lt;listenerId, handler&gt; 中，拥有本次分布式加载所需要存储的信息和操作。 handler 首先获取 master 列表，通过 rpc 调用每个 master（传 listener id）进行 listener 加载和注册。 创建 handler 进行处理过后，service 将数据库中该 listener 的状态改为“注册中”，然后将本次请求线程阻塞，等待 master 回应； master 收到命令后首先获取对应的 jar 包文件和包名类名，进行 class 加载和 listener 对象添加，添加成功后通过 rpc 返回添加结果； handler 若收到了所有 master 的成功返回值，则证明分布式加载成功，唤醒请求线程，service 向数据库写入该 listener 状态为“已注册”，并返回成功结果到前端。 handler 若收到某个 master 抛出的异常，为了保持一致性，通过 rpc 通知所有 master 取消已添加的 listener，分布式加载中断。数据库恢复 listener 状态为“未注册”，并返回错误结果到前端。 请求结束后 service 从 map 中删除对应的 handler。 卸载流程： 用户点击卸载某 listener，service 层每次请求新建一个 unregister handler； 与注册流程类似，handler 首先获取 master 列表，通过 rpc 调用每个 master 卸载该 listener； service 将数据库中该 listener 状态改为“卸载中”，然后阻塞请求线程； handler 收到所有 master 的回应后，唤醒请求线程，状态改为“未注册”，并返回用户结果； 请求结束后 service 从 map 中删除对应的 handler。 Master 端jar包热加载：Listener 的热加载采用 URLClassLoader 实现，示例代码如下： 123456789101112// 根据 jar 包路径和全类名获取用户自定义 Listener 对象public EventListener loadListenerFromJar (String jarPath, String className) throws Exception&#123; // 创建URL，指向jar包的路径 URL url = new URL(jarPath); // 创建一个URLClassLoader URLClassLoader loader = new URLClassLoader(new URL[]&#123;url&#125;); // 加载类 Class&lt;?&gt; clazz = loader.loadClass(className); // 返回类的对象实例 return (EventListener) clazz.getDeclaredConstructor().newInstance();&#125; 加载成功后，首先将 EventListener 对象设置一个唯一 ID 然后添加到 ListenerBus 的 Listener Map 中。 下图是用户成功注册 Listener 的全过程顺序图： image.png 4 时间规划 时间 计划工作 详细说明 2023/07/01 - 2023/07/10 Listener 机制核心功能 包括 ListenerBus、ListenerBusManager 等有关事件事件分发和 Listener 管理的内容 2023/07/11 - 2023/07/20 Event 类实现 将各类 ListenerEvent 进行实现，并在系统原有代码的适当位置生成 2023/07/21 - 2023/07/31 API 接口实现 资源中心有关 listener 管理的各类接口、与数据库的交互 2023/08/01 - 2023/08/20 热加载功能实现 分布式加载与卸载逻辑 2023/08/21 - 2023/08/31 前端界面实现 资源中心添加 Listener 模块界面 2023/09/01 - 2023/09/15 单元测试和集成测试 完成测试工作并修复 bug 2023/09/16 - 2023/09/30 文档编写 完成设计文档和使用文档 5 申请心得在学长的推荐下，我接触到了OSPP开源之夏和 DolphinScheduler 社区。这近一个月的准备时间，我从完全不了解 DS，到慢慢摸清一些代码模块的逻辑，已经收获颇多。我体会到了 DS 开发者们对代码严谨性、可扩展性和健壮性的要求，学习到了大型项目代码的组织方式。感谢孙朝和导师在我调研项目方案时给我的指导，让我对 Listener 机制和分布式开发有了更深刻的理解。如果申请成功，这将是我第一次深入参与开源项目，希望能够借助这个机会学习新的知识、积累新的经验，也真诚希望能够为 DolphinScheduler 社区作出自己的贡献！","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://zyrate.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"学习","slug":"学习","permalink":"https://zyrate.github.io/tags/%E5%AD%A6%E4%B9%A0/"}]},{"title":"RDD分区与并行计算","slug":"RDD分区与并行计算","date":"2023-03-27T16:00:00.000Z","updated":"2023-12-09T12:38:40.790Z","comments":true,"path":"2023/03/28/RDD分区与并行计算/","link":"","permalink":"https://zyrate.github.io/2023/03/28/RDD%E5%88%86%E5%8C%BA%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/","excerpt":"","text":"分区应该是 Spark 中最基础、最核心的概念了，首先搞要清楚为什么要分区。不分区，分布式并行计算就无从谈起。其实哪怕不是分布式，就是如今在一台电脑上 8 个核心也都是标配了，如果把所有的计算任务全都交给一个核心处理便是对硬件资源的极度浪费，效率也十分低下。而要想不浪费，就要采用 并行计算 ，让每一个核心处理一部分任务。而不同的计算需要独立的上下文环境，这就引入了 分区 的概念。每个分区拥有自己的数据和计算函数，当所有的分区计算完毕后，再将它们的结果合并输出。Spark RDD 在逻辑上实现了以上的分区概念。 第一，在 Spark 中几乎所有功能的数据操作都是以 RDD 为单位的（当然还有累加器和广播变量，但是它们都有固定的应用场景），所以它可以看做在 Spark 中的一个“万能数据集”，不论什么数据都能往里面放，不论在哪个场景都可以用，首先明确这一点（其实是为了打破初学者对它的陌生感）。 第二，所谓分区（Partition），简单来讲就是 RDD 在内部将数据分成的不同 切片 ，从不同的数据源读取数据会按照不同的方式进行切片，因此不同的数据源往往会对应不同类型的 RDD 实现类，而每个 RDD 实现都有一个独立的 Partition 实现类来处理数据。在 Partition 实现类中，会用不同的方法存储实实在在的数据。不过这里要搞清楚，RDD 是惰性计算的，只有在执行行动算子后，数据才会在各种不同的 RDD 分区中 计算、接收、传递 ，并不做停留。因此我的理解是，每个 RDD 分区调用它所 依赖的上一级 RDD 的对应的分区计算方法，获得新的分区数据，这本质是一个 链式调用 。这样行动算子会触发数据从读取到一步步计算的链式调用，最终获得计算结果，可以看做分区是固定的，数据一直在变化。 由上所知，在没有发生 Shuffle 的时候，分区数量不变，不同分区之间的计算是 平行的 ，互不干扰，谁快一点谁慢一点都无所谓，重点是它们在同时计算，这就是并行计算。而在遇到了像 groupByKey、orderBy 这样的需要打乱原有数据的方法，分区之间不可能再相安无事了，它们需要相互交换数据，即进行 Shuffle。Shuffle 操作需要数据 落盘 因此十分低效。而当发生 数据倾斜 时，Shuffle 又能够有效地保证计算的 负载均衡 。 第三，RDD 在逻辑上实现了分区，而在集群上实际的计算如何实现的呢？这就要提到 RDD 的任务执行单位：Job、Stage、Task。简单来说，Job 对应一个行动算子，它内部通过 RDD 谱系图 划分 Stage，通常是遇到一个 Shuffle 操作会生成一个新的 Stage。每个 Stage 根据 RDD 的分区数目生成 Task，一个 Task 对应一个分区。注意 Task 运行的结果是目标 RDD 的一个分区，而不是相反。前两个仍然是逻辑上的，真正可以运行的是 Task。Task 是在 Executor 上运行的，每一个物理节点可以起一个或多个 Executor。 所以最终的运行模型是：Driver 端（就是写 Spark 程序的地方）生成 SparkContext 作为和 Spark 框架连接的入口，它会进行 DAG 图构建、Stage 划分、Task 生成等一系列操作，这些操作是在一个节点完成的。而封装好的 Task 会发送给 Yarn 等调度器进行调度，可能会根据 “计算向数据移动” 等准则分发给不同的节点的 Executor，从而进行计算。 知识点： RDD 计算时（行动算子）在 一个分区 内是一个一个数据根据谱系图执行逻辑，即前面一个数据的逻辑全部执行完毕后才轮到下一个数据。分区内部的数据执行是 有序的 ，不同分区之间的数据执行是 无序的 。 MapPartitions 可以以分区为单位进行数据转换操作，但是会将整个分区的数据加载到内存中进行引用，容易出现内存溢出。 窄依赖： 如果新生成的 child RDD 中每个分区都依赖 parent RDD 中的一部分分区，那么这个分区依赖关系被称为 NarrowDependency。 宽依赖： 表示新生成的 child RDD 中的分区依赖 parent RDD 中的每个分区的一部分。","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://zyrate.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"数据可视化","slug":"数据可视化","date":"2023-03-09T16:00:00.000Z","updated":"2023-12-09T12:38:33.785Z","comments":true,"path":"2023/03/10/数据可视化/","link":"","permalink":"https://zyrate.github.io/2023/03/10/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/","excerpt":"","text":"生活中数据可视化无处不在，以前都会有意无意地进行过可视化的工作，但是通过专业化的分析和方法设计出的结果会更能达到可视化的目的，设计过程也会更加得心应手。另外数据可视化有时候并不只是数据的展现，还包含着数据的挖掘。比如看到一批数据，从不同的角度和考量进行可视化可能会从中挖掘出不同的信息。 本文是我学习北京大学袁晓如老师《数据可视化》课程的学习笔记链接：数据可视化 - 华文慕课 - 中文MOOC平台 (chinesemooc.org) 第 1 章概念 数据可视化就是把一些复杂的数据转化成人们能够直接看到并理解的图形或图像，有利于我们更快地识别特征，发现知识。基于计算机的可视化系统通过对数据的视觉表达形式来帮助人们更有效地完成特定任务。 不同的领域、不同的任务、不同的受众的可视化构型是不同的，要做合理、有效的选择。 要考虑计算限制、人类限制、显示限制 总结第一章讲述了可视化的概念、构型和案例，其中构型的选择非常重要，需要考虑不同的领域、任务、受众和限制因素，要在多对矛盾中进行权衡。 第 2 章数据类型 数据集类型 结构化数据：已知数据类型、语义 表格（Tables） 网络（Networks） 场（Fields） 空间/几何（Spatial/Geometry） 多维表（Multidimensional Table） 树形（Trees） 非结构化数据：没有预定的数据模型，如文字、视频、图像。需要转化为结构化数据（NPL、文本挖掘） 数据类型：数据项、链接、属性、位置、网格 属性类型：定类型、定序型、定量型。不同的属性需要用不同的通道表示。看到一个可视化就分析有什么属性，看到属性就要想是什么类型。 表达力和有效性：服从一致性和重要性排序原则，一致性是指，视觉变量和数据属性应该匹配。 2.7 的设计案例有启发意义。 总结这一章主要介绍了各类数据集合数据的类型，目的是强调在可视化过程中，对属性类型的分析是十分重要的，不同类型的属性需要考虑不同的可视化方法，这决定着最后的呈现效果（千差万别）。要培养分析数据属性的思维。 第 3 章数据编码（具体步骤） 符号和通道 符号标记（Marks）：如用圆点和直线代表数据项和连接 点、线、面，（包含、连接、嵌套） 视觉通道（Channels）：符号标记的表现形式，如圆点的颜色 分为以下两个类型，顺序代表有效性从高到低 &lt;定类型&gt; ：空间区域、颜色、运动模式、形状 &lt;定序定量型&gt; ：位置、长度、倾斜度、面积、深度、亮度/饱和度、弧度、体积 考虑视觉通道的五个属性：选择性、关联性、量化性、可序性、容量 史蒂文心理物理强度定律：强度由高到低：饱和度、长度、面积、深度、亮度，感官测试：The eyeballing game (woodgears.ca) 不同类别应该采取的通道排序：image.png 总结直观地感受了各类视觉通道的差异和优缺点，在可视化的时候首先要选择正确的符号和通道，让人们有对数据更加准确的感受。 第 4 章可视化任务与分析 分析三要素： 对象：判断第二章所述的数据类型和属性类型 手段：将第三章所述进行实践 目的：考虑用户需求（什么样的用户） 可视化任务抽象。不同的可视化有不同的任务，这里的任务（功能）是从用户的角度出发，用户为什么需要可视化，该可视化想要用户得到什么信息。要识别任务-数据组合，寻找可能的解决方案。即分析三要素中的目的（行动和对象），行动有以下三个层次（由高到低）：分析、搜索、查询。image.png image.png 分析三要素中的对象，对于不同的对象关心不同的特点： image.png 分析三要素中的手段，考虑可视化构型：视觉编码、交互。（后面讲） 可视化设计验证四层模型：image.png 所谓问题导向就是某个领域的某个问题需要可视化，这时四层模型从外到内进行工作。所谓技术导向就是某个新型的可视化技术出现了，从内到外去寻找可以可视化那些领域问题。 详细学习 4.5 可视化案例。 总结讲述了可视化过程中需要完成哪些分析工作，有哪些要素。从一个顶层抽象的角度阐述了可视化的整个流程。 第 5 章交互 视图操纵的方法 视图随时间变化 重新编码，对于对象 调整参数，不同的小控件（滑块、按钮等） 调整布局、顺序，What、How、Why 重排，对复杂的表格不同的维度（Table Lens）/ 平行坐标 调整对齐方式，堆叠柱形图 过渡动画，在两个状态间做插值平滑 视图分面（Facet） 并置视图（重要）：把两个图放在一起关联 image.png，动态查询是一个经典的例子，快速、增量式和可逆的交互操作。 分隔视图 image.png 叠加视图 image.png 数据约简（Reduce） 过滤：交叉过滤（一个维度变化，另外跟着变） 聚合：空间聚合 不完全互斥可视化系统：Jigsaw总结讲述了可视化中最有趣但却做复杂的交互操作，介绍了视图操纵的几种方法，通过例子体会到不同交互方法的特点和功能，恰当的交互能够给用户带来良好的体验的同时，也能够让用户有更多的发现。 第 6 章光与颜色 颜色表现不止于单一的颜色，还要考虑背景色，和周围的颜色（上下文）。 环境颜色会增加其自身的相反颜色以获得更强的对比 深色增加浅色 红色增加绿色 蓝色增加黄色 感知差异依赖于背景 颜色模型：《CIE 标准观测》 色度图 RGB 色度（三角形） 投影色域 对立色彩 颜色设计准则（经验） 需要考虑上下文，这里的上下文是指除颜色设计之外的各种对象与概念。（比如用户和预算等） 并不是五彩缤纷就是好的，好的设计让信息更吸引人。 颜色包括 &lt;色相、饱和度、亮度&gt; 三个属性，要精确区分。 控制明度，确保易读性 控制色相种类，定义颜色分组，避免太多颜色竞争而混乱，控制“弹出效应” 使用中和背景，最小化 “同时对比效应” 在不同的可视化场景，根据颜色标注的目标不同，颜色的选择也不同。比如飞机上的仪表盘属于需要快速反应的场景，颜色不能太多。 ColorBrewer: Color Advice for Maps (colorbrewer2.org) 网站提供不同的配色方案。 总结在使用颜色的时候需要考虑很多因素，比如对比、色盲等。在设计的时候需要遵循设计准则，让颜色起到增进理解而不是相反的作用。首先要理解颜色的各种属性，精确区分，谨慎选择。 第 7, 8 章表格表格分为平面表格（唯一索引）和多维表格（基于多个键的索引）。 平面数据 表格数据的比较 条形图（可以有不同方向）：要注意基准问题（起始值是否从 0 出发）；要注意是用线性变换还是对数变换，这里的变换是指纵轴单位长度的变化。 折线图：可用【光滑】【连接】和【散点】。要注意如果两个数据点之间连接起来，代表这个属性是可以插值的（比如年龄），如果属性不能插值（比如性别分类）则不能随意连接起来。 散点图：常常用于揭示两个维度之间的相关性。如果有第三维的话，可以将其映射到其他的视觉属性，比如颜色、大小；要注意不要过度绘制，要善用透明度和趋势线。 饼图/圈图：用于表示数据的组成。强调精确数值时用条形图，强调比例时用饼图。 堆叠条形图：将圈图拉直放到坐标系中，比饼图更加直观。 堆叠面积图：将离散的堆叠条形图连接起来。 表示数据的分布的图： 【直方图】与柱状图的区别是没有间隙，横轴是某个属性的区间划分；经验法则：根据数据量的平方根来确定划分区间的数目。 【密度图】 【箱型图】 【小提琴图】 【热力图】 表格可视化经典方法：Table Lens（表格透镜） 高维数据 散点图：点的位置表示两个属性，点的大小、颜色表示更多的属性，但往往表示不了高维（大于三个维度）。 散点矩阵图：每行/列是一个维度，每个单元格绘制两个维度的散点图。下图是 4 个维度两两之间的关系：image.png SPLOM 聚合-热力图：类似于散点图，减少计算量。image.png 相关热力图 Rolling the Dice：两个散点图无缝转换 平行坐标：将 x,y,z 等坐标轴平行放置，可以引入更多的维度，一条连线代表一个数据项，适用于异构数据。当数据量较大的时候，采用 &lt;增加透明度、捆绑、采样&gt; 来解决杂乱问题。 image.png 降维：保留尽可能多的变化，绘制低维空间，主成分分析。 多维缩放：让两两之间在平面的距离尽量正比于在高维空间的距离。常用于文本分析。 地形图表示。 维度嵌套/堆叠： 维度有限，工程数据分析常用 多方法耦合：平行坐标+散点图 其他方法：太多了，看 8.6 节 总结详细讲述了各种图对于表格数据可视化的作用，适当进行选取。 第 9, 10 章网络结构 层次结构（树）：用于有组织结构、分级分类的数据，有谱系树、进化树、搜索树、决策树。 显式树可视化 Reingold-Tilford 布局：类似思维导图 DOI 树（突出焦点）：树节点过多，只强调部分节点（增大），或用三角形代表不重要的子树。 双曲线树（突出焦点）：面向大规模的层次结构数据，全体数据可见，焦点放大。image.png 隐式树可视化：看不见树的结构，但是树的内部关系。较重要的是包含式非显式布局。其中最重要的是【树图】（Treemap）：切分空间，节点为长方形，节点面积代表相应属性。 树比较可视化：用柱状图进行树之间的比较。 图的可视化 两种主要类型的任务：【基于属性】、【基于拓扑】。 显式图形式：image.png，布局标准如下，减少用户阅读的干扰（不用全部满足）： 最小化边交叉 最小化相邻接点的距离 最小化绘图区域 边长度统一 最小化边弯曲 最大化不同边之间的角距离（过多的锐角不容易分辨） 宽高比约为 1（不太长也不太宽） 对称性（类似的图结构看起来相似） 矩阵形式：即图的邻接矩阵。非常适合邻域相关的任务，不适合路径相关的任务；节点的顺序很重要，排序后可能会发现规律。 混合显示与矩阵形式：NodeTrix。image.png 力导向布局算法：边=弹簧，点=互斥磁铁，算法开销较大 image.png 总结不同可视化方法之间需要进行取舍，为了相应的目的，可能会降低对另外一部分性能的支持。不同的可视化方法可以混用，可能达到更好的效果。 第 10 章时间序列时间序列数据就是其中一个变量是时间的数据，也可以说是高维中一个维度是时间的数据。vcg.informatik.uni-rostock.de可视化方法有： 缩略组图（Small Multiples）：在单个页面上显示的呈缩略图大小的图形集，表示单个现象的不同方面（不同时间）。也适用于多变量（多维）显示。比如新冠晴雨表。 形态替换：将时间视为隐藏的维度，为每个时间帧生成一个可视化，然后播放动画，用户可以进行追踪（可以加上轨迹）。Gapminder Tools 。但是该方法有一个问题是变化盲视，即人们没有注意到场景中可见元素的变化，需要根据具体情况解决（如增加视觉编码）。 时间序列图：将横坐标规定为时间，纵坐标为属性（可以有多个，不同编码也可以嵌套堆叠）。对于多个时间序列的比较，有以下几种方法： 简单线图：多条不同的线在一起。 编织线图：交替地根据数值的大小进行前后排列。 计算曲线焦点并垂直切割曲线面积，按照深度排序优先绘制最高部分面积。 换句话就是：高个子永远在后面。 image.png 缩略视图：见前文。 水平线图：解决缩略视图在高度较小的情况下空间利用问题。（压缩高度，保留精度） 堆叠线图：把不同的线型堆叠在一起。 螺旋图：更好的体现周期性；注意比例和标注。 像素驱动方法：每个像素代表一个时间点。 一行行/一列列排布 用填充曲线（Peano-Hilbert）：时间上相近的，空间上也相近。 时间曲线（Time Curves）：时间顺序的排列并不是规则的（水平的），而是根据内容相似性进行分布（弯曲）。曲线的形状可以表示相应的演变。（高维映射到低维 / 多维缩放） 主题河流（ThemeRivers）：表示文本随时间的变化。 从左到右流经时间，类似堆叠曲线图 image.png 总结时间在可视化里面可以看做高维中的一个带有先后顺序的属性，所以时间序列可视化都是在视觉上具有一定的连续性，这种联系可以让人们更直观地感受时间的流动与事物的变化。 第 12 章地图 使用地图的原则和任务 原则：当空间关系被着重强调时使用地图 任务：寻找地点/特征、寻找从 A 到 B 的路径、辨认与地点相关的属性、基于地点比较属性 地图投影 - 将地球展开 要考虑的属性：面积、形状、方向、方位、距离、尺度 投影方法 墨卡托投影：投影至一个包裹着地球的圆柱上，再展开成平面。所有的经纬线都是直线前垂直相交；方位准确，面积不准确。 方位角等距投影：确定航线走向。image.png 温格尔投影：最小化三种失真（面积、方向、距离）。image.png 锥形投影 阿尔博斯等面积投影：正确显示面积 复合投影 其他投影方式：Extended geographic projections for d3-geo. (github.com) 区域分布地图 用区域填充的颜色或图案来表示数值，如美国大选地图 问题：具有误导性，因为某区域面积的大小可能与数值没有关系 解决方法：同一个颜色用深浅区分数值大小，或加入其他编码（如密度） 等高线地图：用来表述在空间中的数值分布，特别是数值之间的过渡。 统计/变形地图：舍弃了地理区域的真实面积，而用数值大小来决定面积（缩放），但保留了原地理区域之间的方位、接壤等信息。 比例标识地图：保留原地理区域的真实面积，而采用添加圆圈（或其他图形），用它的的大小或其他定量属性来代表数值的大小。 流图：用于表示数据在不同地域之间的流动。（交通部门） 地铁地图：采用伦敦地铁图方式。线路水平、垂直或 45 度，车站间等距。 总结地图可视化都是以真实的地理区域为基础，而自然地理区域的面积、方位是固定的，想要表示的数据又往往与面积、方位无关，这就需要考虑如何规避掉区域自然属性对可视化目的的干扰。 第 13 章经验法则大多数情况下简单的、经验性的、探索性的但不是很准确的原则，体系不完整。 慎用 3D（但是技术在发展） 屏幕不是三维的，更适合 2D 信息 人对深度的判断不够精确 会产生遮挡，无法了解相关关系，带来时间成本 透视会引起失真 3D 下的文本会倾斜，造成认知负荷 慎用 2D：能用 1D 的列表就不要用 2D，1D 更适合查找、排序任务。在可视化里面，能简则简，不要追求复杂（越简洁越有效）。 慎用多视图的简单组合：缺乏数据内在逻辑的关联，无法提供深度探索。多视图需要有侧重点，图与图之间要紧密联系，有紧密的交互。（有机结合） 可见性重于记忆：如果能够通过不同视图直接对比，就不要采用动画，因为动画要求用户记忆，带来负担。 分辨率优先：沉浸感依赖于分辨率。简单说就是优先提高分辨率，而不是整花里胡哨的东西。 概览优先，缩放与过滤，细节按需呈现：【大量数据 -&gt; 展示概览，忽略细节 -&gt; 提供提示 -&gt; 用户定位到感兴趣的地方 -&gt; 放大 -&gt; 涌现细节】 交互响应不可缺少：即时反馈非常重要，如不能即时，应告知用户处理进度或先显示一部分。 黑白情况下的可用性：可视化在黑白情况下依然有效。借用亮度、色度、饱和度等通道。 功能重于形式：坚持有效性优先原则，考虑用户需求。 总结经验法则是贯穿于所有可视化技术的原则，在进行可视化工作之前和完成之后，都可以对照经验法则检验工作是否得当。","categories":[],"tags":[{"name":"学习","slug":"学习","permalink":"https://zyrate.github.io/tags/%E5%AD%A6%E4%B9%A0/"}]},{"title":"完全分布式Hadoop集群在虚拟机CentOS7上搭建——过程与注意事项","slug":"hadoop centos","date":"2019-12-06T16:00:00.000Z","updated":"2024-03-10T09:23:28.000Z","comments":true,"path":"2019/12/07/hadoop centos/","link":"","permalink":"https://zyrate.github.io/2019/12/07/hadoop%20centos/","excerpt":"","text":"搭了几次集群后，从什么都不懂到懂了点皮毛，有了些心得体会，写在前面，不一定对但我是这么理解的： master = Namenode + SecondaryNamenode slave = Datanode 每个 slave 节点都要保证能与 master相互免密 SSH 连接，但 slave 节点之间无所谓 其实不管是 master 节点还是 slave 节点，它们的配置都可以是一样的，只是后期确定一个 master，让所有的 slave 都能与其互通，然后格式化 namenode，这样它逻辑上就是 master 了。所以只需要配置一台虚拟机当 master，之后需要几个 slave 就克隆几台就行了，只不过需要改一些配置。(不想克隆的话把配置好了的 hadoop 文件夹远程复制到 slave 上，再配一下环境变量和 hosts) 我有两台云服务器，原来幻想着建一个虚拟机当 master，把那两台服务器当 slave，后来发现太天真了，slave 和 master 之间必须能够相互访问、传数据，而云服务器连本地主机 IP都 ping 不通，怎么集群？所以是不能把云服务器和虚拟机混用的。 需要配置的地方：**/etc/sysconfig/network-scripts/ifcfg-ens33**(配 IP)、jdk、hadoop/etc/hadoop、hadoop/tmp(自建)、**/etc/profile、/etc/hosts、~/. ssh**(免密登录)、防火墙 本次集群环境：==VMware Workstation Pro 14==、==CentOS 7==、==jdk1.8.0_161==、==hadoop-2.10.0== 下面记录一下配置搭建过程，首先配 master，打开 Vmware14，装一台全新的 CentOS 7，我推荐装最简版，之后需要什么功能才下载，不然太臃肿了。安装过程省略，用 root 登录 配置 IP用以下命令查看和配置本机 IP，保证和主机互 ping 通ip addrvi /etc/sysconfig/network-scripts/ifcfg-ens33 12345678910111213141516171819202122TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=static (这里）DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=795a782a-9240-4230-95fc-bcc33bdbbd97DEVICE=ens33ONBOOT=yes （这里）（下面按自己情况）IPADDR=192.168.117.101NETMASK=255.255.255.0GATEWAY=192.168.117.2DNS1=114.114.114.114ZONE=public 重启服务：systemctl restart network.service 改主机名，配置 hosts 文件其实主机名改不改无所谓，hosts 改了就行了，但为了便于区分，每台节点的主机名还是改一下吧。改主机名：hostnamectl set-hostname master执行完之后发现并没有变，其实已经改了 在这里插入图片描述 重启一下或者重连一下 SSH 就会变了 在这里插入图片描述 配置 hosts：vi /etc/hosts自己需要几个节点就配几个，之后如果还加节点的话，那 master 上的 hosts 也要更新 在这里插入图片描述 关闭防火墙这一步很重要，不关的话会出现很多问题systemctl stop firewalld.servicesystemctl disable firewalld.service 安装 JDK把网上下载的==jdk-8u161-linux-x64. tar. gz==文件传到 /usr/local/java 目录下，这个目录随意。进到目录后，解压即可：tar -zxvf jdk-8u161-linux-x64.tar.gz环境变量之后和 Hadoop 一起配。 安装 Hadoop与 JDK 类似，把网上下载的==hadoop-2.10.0. tar. gz==文件传到 /usr/local 目录下，进目录之后解压：tar -zxvf hadoop-2.10.0.tar.gz改名：（为了方便）mv hadoop-2.10.0 hadoop 配置环境变量执行以下命令：vi /etc/profile在文件的最后加上以下配置：（根据自己路径）（这个配置可能有些不优美但绝对没错） 1234export JAVA_HOME=/usr/local/java/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATHexport HADOOP_HOME=/usr/local/hadoop/export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 更新环境变量：source /etc/profile验证：java -version 在这里插入图片描述 配置 Hadoop 文件进入 Hadoop 的配置目录cd /usr/local/hadoop/etc/hadoop可以看到有许多配置文件，一个一个配置 hadoop-env. sh将文件靠前面的export JAVA_HOME=$&#123;JAVA_HOME&#125;中的${JAVA_HOME}改成自己的 Java 路径 在这里插入图片描述 yarn-env. sh将文件靠前面的# export JAVA_HOME=/home/y/libexec/jdk1.6.0/前的 # 去掉，然后改成自己的 Java 路径 在这里插入图片描述 core-site. xml用下面的代码替换 12345678910&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site. xml用下面的代码替换 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site. xml用下面的代码替换 123456789101112131415161718192021222324252627&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:18040&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:18030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:18025&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:18141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:18088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 这里端口号不是死板的。 mapred-site. xml 一开始是没有这个文件的，需要从它给的模板复制一个：cp mapred-site.xml.template mapred-site.xml用下面的代码替换 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slaves打开后，有几个从节点就写几个，名字必须和 hosts 里的一致，我这里的是 slave 和 slave2，slaves 文件只对 master 节点有用，如果新增从节点的话，这个文件也要改。在这里插入图片描述 新建 Hadoop 数据目录添加 hdfs-site. xml 文件里配置的并不存在的数据目录：目录结构是这样的：/usr/local/hadoop/==tmp==/usr/local/hadoop/tmp/==dfs==/usr/local/hadoop/tmp/dfs/==data==/usr/local/hadoop/tmp/dfs/==name==高亮的是新建的目录。 至此，一台 master 节点的全部配置就完成了，接下来 slave 想克隆几台就克隆几台，我这里克隆了两台，距离分布式集群完全完成还差：配克隆后 IP、配免密登录 克隆虚拟机关闭 master右键 master 虚拟机 –&gt; 管理 –&gt; 克隆根据克隆向导一路继续，注意，一定要选 创建完整克隆！ 在这里插入图片描述 配置克隆后的 IP这里跟前面配 IP 的方法一样，就不赘述，但是一定要保证每一台 slave 的主机名和 IP 与 master 节点里 hosts 文件里的主机名和 IP 保持一致。 配置 SSH 免密登录这一步也很重要，不配的话每次都要输密码，过于麻烦也不现实。配置免密登录基本原理就是，把自己的公钥（一串字符串）复制到别人的 authorized_keys 文件里，当一个主机要 SSH 连接另一个主机时，如果本机的 authorized_keys 文件存有那个主机的公钥，那么就不需要输密码，否则，就连主机 SSH 自己都要输密码。所以，我们的目的就是： 让 master 节点自己对自己、自己对所有slave 免密 让每一台 slave 节点都自己对自己、自己对 master 免密 首先进入 master，以 root 登录。密钥和 authorized_keys 的所在目录是 ~/.ssh 下面，但初次配置会发现没有这个目录，这时我们只需随意 SSH 登录一个节点，再退出来就会出现这个目录了。进入 cd ~/.ssh执行 ssh-keygen -t rsa 生成密钥（连按 3 次回车），对自身免密 ssh-copy-id master对所有从节点免密 ssh-copy-id slave，依次执行对 slave2、slave3 等等，这个命令其实就是自动把公钥复制到指定主机的 authorized_keys 文件中去。 然后分别进入几个从节点，进行类似的操作。这时候从节点已经有 ~/.ssh 目录了，进入后执行 ssh-keygen -t -rsa 命令，用 ssh-copy-id 命令分别对自己、对 master 进行免密。 可以验证，如果免密成功，SSH 另一个主机是不需要密码就能登录的。 启动 Hadoop 集群至此，完全分布式集群的所有配置就大功告成了，满怀着激动的心情启动吧！==这时所有的节点必须开机！==首先要在 master 节点上进行格式化：hdfs namenode -format 在这里插入图片描述 看到这句话就说明格式化成功了（当然这句话有时候并不能说明什么~_ ~）启动 Hadoop！start-all.sh 在这里插入图片描述 启动好了之后，分别在所有节点执行jps命令以验证成功与否：当 master 节点为： 在这里插入图片描述 slave 节点都为： 在这里插入图片描述 就说明运行成功了。 进一步验证在宿主机浏览器上输入：(master的IP地址):50070看到： 在这里插入图片描述 在这里插入图片描述 将 50070 改为 18088： 在这里插入图片描述 就大功告成了！关闭集群：stop-all.sh 特别注意 如果集群启动后，jps 命令给出的结果与正确的不符，比如少了一个或多了一个，那么大多就需要重新格式化，重新格式化不是再执行一遍 hdfs namenode -format 那么简单，需要把所有节点下的 /usr/local/hadoop/tmp 目录的结构按照上面说到的方式重新建立，即把之前的都删了，重新建文件夹。（重建之前记得关闭集群）之后再格式化、启动。 如果启动后，在宿主机上无法访问虚拟机 IP 及端口，可能是虚拟机防火墙没关，也可能是宿主机的防火墙规则设置问题。","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://zyrate.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"hadoop","slug":"hadoop","permalink":"https://zyrate.github.io/tags/hadoop/"}]},{"title":"#{}的困惑 - Mybatis中的参数传递","slug":"mybatis1","date":"2019-09-14T16:00:00.000Z","updated":"2024-03-10T09:17:15.000Z","comments":true,"path":"2019/09/15/mybatis1/","link":"","permalink":"https://zyrate.github.io/2019/09/15/mybatis1/","excerpt":"","text":"Mybatis 的初学者一定对 sql 映射文件中许多的#{}符号感到陌生。其实我们都知道，它可以理解为一个占位符，把传递过来的数据放到 sql 语句中，但有些时候它又我觉得困惑，最后大致了解了一下，下面说一下我所学到的知识。 #{}符号大致有以下三种用途： 1. 获取传递来的原始类型参数我们经常将操作方法定义在一个接口里面。当我们在接口里定义诸如 1User getUser(Integer id); 的方法时，我们需要向 Mybatis 传递一个整型参数 id，这时候，许多教程上的映射文件就是这么写的： 123&lt;select id=&quot;getUser&quot; resultType=&quot;com.User&quot;&gt; select * from user where id = #&#123;id&#125;&lt;/select&gt; 那我们就会情不自禁地想了：#{id}就是代指我传递过来的参数呗，也就是说，参数名是啥#{}就填啥呗，但事实是不论你写 #&#123;name&#125;、#&#123;gender&#125;、#&#123;fuck&#125; 程序都能正确运行。这是因为，当只有一个参数传递时，Mybatis 不会对其作特殊处理，不管你大括号里写的什么，它都会用参数替换。 它真的就只是一个占位置的。 那么多个参数呢？ 1User getUser(Integer id, String name); 如果你这么写： 123&lt;select id=&quot;getUser&quot; resultType=&quot;com.User&quot;&gt; select * from user where id = #&#123;id&#125; and name = #&#123;name&#125;&lt;/select&gt; 就会发现刺眼的报错信息了。实际上，当有多个参数 (不止是原始类型)传递时，Mybatis 会对它们做特殊处理——封装成 Map，其中键是 param1, param2, param3… 类推，键才是传递过来的参数，我们在#{}中写的需是键，也就是 #&#123;param1&#125;, #&#123;param2&#125;，或是它们的下标 #&#123;0&#125;, #&#123;1&#125;。但这样也未免太麻烦，所以现在主要采用写 命名参数 的方法： 在声明接口时用注解命名参数，就相当于自己命名参数的键名： 1User getUser(@Param(&quot;id&quot;)Integer id, @Param(&quot;name&quot;)String name); 这样一来上面的报错信息就消失了。另外，如果传入的是一个 Collection 或数组类型的参数，Mybatis 也会把它封装成 Map，这时候如果不自己命名的话键是固定的：”collection”或”list”或”array”。 2. 获取对象中的属性值这个就好理解了，当参数只有一个并且是个 JavaBean 对象 POJO 时，#{}里面就写这个对象的属性名，Mybatis 就能把它取出来。例如：JavaBean 123456public class User &#123; Integer userid; String username; Character gender; //...&#125; 接口 1User getUser(User user); sql 123&lt;select id=&quot;getUser&quot; resultType=&quot;com.User&quot;&gt; select * from user where id = #&#123;userid&#125; and name = #&#123;username&#125;&lt;/select&gt; 3. 获取 Map 中的值当我们传入一个 Map 后，#{}里写键，那么 Mybatis 就会取出值来。 不得不说，Mybatis 真的是既简便又强大。","categories":[],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://zyrate.github.io/tags/mybatis/"},{"name":"mysql","slug":"mysql","permalink":"https://zyrate.github.io/tags/mysql/"}]},{"title":"IntelliJ IDEA搭建SSH框架 maven项目的过程总结","slug":"ssh maven","date":"2019-08-10T16:00:00.000Z","updated":"2024-03-10T09:15:42.000Z","comments":true,"path":"2019/08/11/ssh maven/","link":"","permalink":"https://zyrate.github.io/2019/08/11/ssh%20maven/","excerpt":"","text":"第一篇博客需要了解 maven 在 idea 里的使用及作用本步骤使用 spring 的 XML 配置方式搭建，不使用注解 创建 maven 项目New Project -&gt; 在 create from archetype 上打勾 -&gt; 选择 maven-archetype-webapp -&gt; Next 创建maven项目 GroupId 是包名，一般是 com. 公司名或姓名，Artificial 是项目名 在这里插入图片描述 这里可以用默认的 maven 仓库，但一般在别的盘自建一个仓库，参考别人的教程 在这里插入图片描述 然后 next -&gt; finish 在这里插入图片描述 如果是第一次创建，会等待时间稍微长一些，出现 BUILD SUCCESS 就行了，右下角出现的提示我一般点 Enable Auto-Import. 在这里插入图片描述 配置 Tomcat在这里插入图片描述 在这里插入图片描述 在 Server 面板配置上本地的 tomcat 服务器，随便命个名，在 Deployment 面板里点击加号 Artifact 在这里插入图片描述 选中有 exploded 的一个，OK，然后下面的 Application context 可以随意写，这里我写上项目名，也可以不写，只留一个/，这样的话在地址栏只需输入 localhost:8080/...。之后 OK 确定就行了。 在这里插入图片描述 整合与样例代码配置 pom. xml项目创建好了，接下来开始一系列的配置，我这里用的是==Spring4 + Struts2 + Hibernate4==, 注意有时候版本不同操作也会不同。接下来就是配置相关的依赖： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;4.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;4.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;4.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- hibernate --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;4.1.1.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- struts2 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.struts&lt;/groupId&gt; &lt;artifactId&gt;struts2-core&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--struts与spring的整合--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.struts&lt;/groupId&gt; &lt;artifactId&gt;struts2-spring-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql数据库 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt; 配置 web. xml在 web. xml 里加入以下代码： 1234567891011121314151617&lt;!-- Spring的核心监听器--&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- struts2的核心过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;struts&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;struts&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 建立目录结构在 src 目录下新建 java 和 resources 目录，分别右键Mark Directory as Sources Root 和 Resources Root。此后所有的配置文件最好都建在 resources 目录下面，不然可能识别不到。 在这里插入图片描述 然后在 java 目录下建好 action、service、dao 和 domain目录，分别是 Web 层、业务层、持久层和实体。 在这里插入图片描述 Spring 整合 Struts2所谓的 SSH 框架搭建不过是 Spring、Struts2、Hibernate 三个框架的整合，要整合就有个先后顺序，我首先整合 Spring 和 Struts2，然后再整合 Spring 和 Hibernate。我的理解是：Spring 在其中就像一个舞台，给了其他框架施展的空间；它也是一个管理者，让我们程序员学会变“懒”，专注重要的事情。 我这里准备用 添加用户 这个操作来做演示，既然先整合 Struts，就暂时不管数据库，只去看看这个操作能否按照我们想要的步骤执行。 创建样例类按照层层递进（持久层、业务层、web 层）的顺序，首先要有一个用户实体类，在 domain 下创建 User.java： 123456789101112package domain;/** * 用户实体 */public class User &#123; //这里要用包装类型 private Integer id; private String name; private Character gender; //getter... //setter...&#125; 然后是 User 的持久层，在 dao 下创建 UserDao.java： 1234567891011121314package dao;import domain.User;/** * 用户持久层 */public class UserDao &#123; /** * 添加用户 * @param user */ public void addUser(User user)&#123; System.out.println(&quot;UserDao.addUser&quot;); &#125;&#125; 接着是 User 的业务层，在 service 下创建 UserService.java： 123456789101112131415161718192021package service;import dao.UserDao;import domain.User;/** * 用户业务层 */public class UserService &#123; //用Spring注入 private UserDao userDao; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; /** * 添加用户 * @param user */ public void addUser(User user)&#123; System.out.println(&quot;UserService.addUser&quot;); userDao.addUser(user); &#125;&#125; 最后是 User 的 action，在 action 下创建 UserAction.java： 123456789101112131415161718192021222324252627282930package action;import com.opensymphony.xwork2.ActionSupport;import com.opensymphony.xwork2.ModelDriven;import domain.User;import service.UserService;/** * 用户相关的Action *///struts2的标准代码public class UserAction extends ActionSupport implements ModelDriven&lt;User&gt; &#123; //模型驱动需要用的对象，也就是User对象 private User user = new User(); @Override public User getModel() &#123; return user; &#125; //Spring注入 private UserService userService; public void setUserService(UserService userService) &#123; this.userService = userService; &#125; /** * 添加用户方法 */ public void addUser()&#123; System.out.println(&quot;UserAction.addUser&quot;); userService.addUser(user); &#125;&#125; 我们在 jsp 页面提交添加用户的表单后，提交到这个 action 的 addUser 方法，然后一路调用，在控制台显示出不同方法的信息，就算成功了，但现在还差许多配置，接着往下。 配置 spring. xml在 resources 目录下建立 spring.xml 文件，文件头模板如下： 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt; &lt;/beans&gt; 在其中管理以下 bean： 1234567&lt;bean id=&quot;userDao&quot; class=&quot;dao.UserDao&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;userService&quot; class=&quot;service.UserService&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;userAction&quot; class=&quot;action.UserAction&quot;&gt; &lt;property name=&quot;userService&quot; ref=&quot;userService&quot;/&gt; &lt;/bean&gt; 配置 struts. xml在 resources 目录下新建 struts.xml： 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE struts PUBLIC &quot;-//Apache Software Foundation//DTD Struts Configuration 2.3//EN&quot; &quot;http://struts.apache.org/dtds/struts-2.3.dtd&quot;&gt;&lt;struts&gt; &lt;package name=&quot;随便&quot; extends=&quot;struts-default&quot; namespace=&quot;/&quot;&gt; &lt;action name=&quot;user_*&quot; class=&quot;userAction&quot; method=&quot;&#123;1&#125;&quot;/&gt; &lt;/package&gt;&lt;/struts&gt; 至此，Spring 和 Struts2 的整合就大功告成了，只剩一个 jsp 页面来验证是否成功了！ 编写页面我们可以修改 idea 自动生成的 index.jsp，用 struts 的标签写一个添加用户页面： 123456789101112131415161718192021222324252627&lt;%@ taglib prefix=&quot;s&quot; uri=&quot;/struts-tags&quot; %&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;body&gt;&lt;h2&gt;添加用户&lt;/h2&gt;&lt;!-- 注意action的写法 --&gt;&lt;s:form action=&quot;user_addUser&quot; method=&quot;POST&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户ID&lt;/td&gt; &lt;td&gt;&lt;s:textfield name=&quot;id&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;td&gt;&lt;s:textfield name=&quot;name&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;性别&lt;/td&gt; &lt;td&gt;&lt;s:textfield name=&quot;gender&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;&lt;input type=&quot;submit&quot; value=&quot;添加&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/s:form&gt;&lt;/body&gt;&lt;/html&gt; 运行测试我们启动 Tomcat，启动好之后，在浏览器地址栏输入 localhost:8080/SSH_Demo/index.jsp输入用户数据后，按添加按钮提交。这时，地址栏应该变成 http://localhost:8080/SSH_Demo/user_addUser我们回到 idea 编辑器，看到控制台输出以下信息就成功了： 在这里插入图片描述 Spring 整合 Hibernate修改 Dao 层代码之前的 UserDao 继承 HibernateDaoSupport 类，调用 getHibernateTemplate 方法获取模板，直接 save 要保存的对象就可以了。 1234567891011121314151617package dao;import domain.User;import org.springframework.orm.hibernate4.support.HibernateDaoSupport;/** * 用户持久层 */public class UserDao extends HibernateDaoSupport&#123;//继承这个类很方便 /** * 添加用户 * @param user */ public void addUser(User user) &#123; System.out.println(&quot;UserDao.addUser&quot;); this.getHibernateTemplate().save(user); &#125;&#125; 创建数据库和属性文件 我们可以用数据库管理工具新建一个数据库，我这里命名为 ssh_demo，不用创建表，hibernate 会帮我们创建。 为了方便，jdbc 的各种配置放在属性文件里，在 resources 目录下创建 jdbc.properties，用户名密码什么的填自己的。1234jdbc.driverClass=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/ssh_demo?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=GMT%2B8jdbc.username=rootjdbc.password=2018 创建 Hibernate 映射文件在 resources 下创建 user.hbm.xml，有了这个文件 Hibernate 才能把对象和数据库表对应起来。1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-//Hibernate/Hibernate Mapping DTD 3.0//EN&quot; &quot;http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd&quot;&gt;&lt;hibernate-mapping&gt; &lt;class name=&quot;domain.User&quot; table=&quot;user&quot;&gt; &lt;id name=&quot;id&quot; column=&quot;id&quot;&gt; &lt;!-- 自动增长 --&gt; &lt;generator class=&quot;native&quot;/&gt; &lt;/id&gt; &lt;property name=&quot;name&quot; column=&quot;name&quot; length=&quot;20&quot;/&gt; &lt;property name=&quot;gender&quot; column=&quot;gender&quot;/&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 配置 spring. xml接下来的配置有点繁琐，但都是按部就班的比较固定的步骤，一步步来就行。下面的操作写在 spring. xml 里面 引入外部的属性文件1&lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; 配置连接池123456&lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;$&#123;jdbc.driverClass&#125;&quot;/&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;user&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt;&lt;/bean&gt; 配置 Hibernate 的相关属性（通过注入 sessionFactory）12345678910111213141516171819&lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate4.LocalSessionFactoryBean&quot;&gt; &lt;!--连接池--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!--Hibernate属性--&gt; &lt;property name=&quot;hibernateProperties&quot;&gt; &lt;props&gt; &lt;prop key=&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;prop key=&quot;hibernate.show_sql&quot;&gt;true&lt;/prop&gt; &lt;prop key=&quot;hibernate.format_sql&quot;&gt;true&lt;/prop&gt; &lt;prop key=&quot;hibernate.hbm2ddl.auto&quot;&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!--加载hibernate中的映射文件--&gt; &lt;property name=&quot;mappingResources&quot;&gt; &lt;list&gt; &lt;value&gt;user.hbm.xml&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 将 sessionFactory 注入 UserDao123&lt;bean id=&quot;productDao&quot; class=&quot;com.zyr.ssh.dao.ProductDao&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot;/&gt;&lt;/bean&gt; 配置事务管理器123&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate4.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot;/&gt;&lt;/bean&gt; 开启事务注解 &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; 在 UserService 类上加一个注解 @Transactional ==大功告成 ！== 运行测试同样的，运行 Tomcat，在浏览器地址栏输入 localhost:8080/SSH_Demo/index.jsp，输入用户 ID、姓名、性别后提交。再看看数据库里是不是已经有一个 user 表，并且插入了一条数据啦。至此 SSH 框架的初步搭建已完成。 这是我第一次写博客，关于 ssh 框架也正在学习过程中，有什么不妥或错误的地方还望大家谅解与指教！","categories":[],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://zyrate.github.io/tags/ssh/"},{"name":"idea","slug":"idea","permalink":"https://zyrate.github.io/tags/idea/"},{"name":"maven","slug":"maven","permalink":"https://zyrate.github.io/tags/maven/"}]}],"categories":[],"tags":[{"name":"go","slug":"go","permalink":"https://zyrate.github.io/tags/go/"},{"name":"mit6824","slug":"mit6824","permalink":"https://zyrate.github.io/tags/mit6824/"},{"name":"分布式","slug":"分布式","permalink":"https://zyrate.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"java","slug":"java","permalink":"https://zyrate.github.io/tags/java/"},{"name":"mit6830","slug":"mit6830","permalink":"https://zyrate.github.io/tags/mit6830/"},{"name":"学习","slug":"学习","permalink":"https://zyrate.github.io/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"hadoop","slug":"hadoop","permalink":"https://zyrate.github.io/tags/hadoop/"},{"name":"mybatis","slug":"mybatis","permalink":"https://zyrate.github.io/tags/mybatis/"},{"name":"mysql","slug":"mysql","permalink":"https://zyrate.github.io/tags/mysql/"},{"name":"ssh","slug":"ssh","permalink":"https://zyrate.github.io/tags/ssh/"},{"name":"idea","slug":"idea","permalink":"https://zyrate.github.io/tags/idea/"},{"name":"maven","slug":"maven","permalink":"https://zyrate.github.io/tags/maven/"}]}